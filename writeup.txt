1. What is used as the context, or the input to the first decoder?

The input to the first decoder, the context, is the hidden state that came out as an output from the second encoder after the last time step batch of the inputs are processed in the first and second encoder.
 
2. Can this framework accept variable length of input video frames? Assuming the number of input frame is 10, can we possibly predict more than 10 output frames? If no, why not? If yes, please tell us how.

Yes, this framework accepts variable lengths of input video frames since the encoder and decoder are designed to take inputs and outputs of variable lengths. Predicting more than ten output frames can be challenging because the number of ConvLSTMs used in the encoder and decoder is small to remember long-term dependencies.

3. Why do we use the sigmoid layer in forget gate? What ability does the sigmoid layer carry in this case?

The sigmoid layer has the ability to give a number between 0 and 1, which can be treated as a probability for any given input. Therefore, the sigmoid layer in the forget gate is used to output a probability between 0 and 1 to keep the cell state from the previous time step instead of updating it with a new value.


4. Which of the following architecture better capture spatio-temporal correlations: fully connected LSTM vs. ConvLSTM? What component is the inferior architecture lacking in comparison with the superior one?

ConvLSTM is a better architecture in capturing patio-temporal correlations that fully connected LSTM. Comparing spatial patterns observed at multiple times requires processing multiple images at different time steps, which requires a huge number of parameters for the fully connected LSTM to learn when compared to ConvLSTM. Fully connected LSTM lacks filters and convolution operations, which are part of ConvLSTM, to reduce the number of parameters required to learn when processing images across multiple time steps.


5. By examining visualizations, you will find that the predictions are fading away through time. Please write a paragraph in the report to explain why the generated frames are "fading away".

The generated frames are "fading away" because the number of ConvLSTMs used in the encoder and decoder is not enough to remember dependencies across longer future time steps, which results in some amount of information not being presented correctly when generating predictions across longer future time steps.


Extra Credit:

Stacking more ConvLSTMs in both the encoder and decoder to remember dependencies across longer future time steps, or using attention models could help solve the fading away of the predictions through time.


