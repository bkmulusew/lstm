{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66376083",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6a71f8bc6d4956b8c3997c3133442f6",
     "grade": false,
     "grade_id": "cell-a6f327a43f356a64",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Project 2: Predict the Next Frames\n",
    "\n",
    "Recurrent Neural Network is developed for processing variable length sequences of inputs, e.g., videos. Since you have already learned about RNN in the class, now you will apply it to an interesting task: predict the next frames of a video. Specifically, given the first half of a video, you will use RNN to predict the second half. \n",
    "\n",
    "Submission Instructions:\n",
    "Upload a ZIP archive with the completed jupyter notebook and a report to BlackBoard before the deadline.\n",
    "- Notebook: your implementation of RNN.\n",
    "- Report: everything else, containing the analysis of your code and qualitative/quantitative results.\n",
    "\n",
    "This project is due by **11:59pm EDT on October 31st 2021**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7ec02d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4d0e1afd34862958d43007ca4e56309",
     "grade": false,
     "grade_id": "cell-27b14563a95afb42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. Package\n",
    "\n",
    "Let's first import all the packages that you will need.\n",
    "\n",
    "- **torch, torch.nn, torch.nn.functional** are the fundamental modules in pytorch library, supporting Python programs that facilitates building deep learning projects.\n",
    "- **torchvision** is a library for Computer Vision that goes hand in hand with PyTorch\n",
    "- **numpy** is the fundamental package for scientific computing with Python programs.\n",
    "- **PIL, matplotlib** are libraries to plot graphs and save images in Python.\n",
    "- **os, random** are the standard modules in Python.\n",
    "- **argparse** is a library for writing user-friendly command-line interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "755bf3d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50eb78c1efb2c44da871e5a0adf9c91e",
     "grade": true,
     "grade_id": "cell-ef2268cfa3e8582d",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import packages successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import random\n",
    "from project2_utils import *\n",
    "\n",
    "# Speicify which gpu to use\n",
    "gpu_ids = \"0\" # You can change it to other gpu id, e.g., \"1\" or \"2\" if working on a multi-gpu machine\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_ids\n",
    "\n",
    "print(\"Import packages successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cae42c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11e0fefc249865e2d638d9a9ad54e477",
     "grade": false,
     "grade_id": "cell-8c4c3f58a8b7c83d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Dataset\n",
    "\n",
    "You will use a moving digits dataset for this project.\n",
    "\n",
    "Let's first use ``torch.utils.data.Dataset`` to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ba81f3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dded04fc6e9652731dafa12dcec71726",
     "grade": true,
     "grade_id": "cell-42087e977ab1e41c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 10000\n",
      "Number of testing examples: 1000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the root directory of data \n",
    "root = '/u/cs298/project2/data'\n",
    "\n",
    "# load dataset\n",
    "train_set = MovingDigits(os.path.join(root, 'train.gz'), is_train=True)\n",
    "test_set = MovingDigits(os.path.join(root, 'test.pt'), is_train=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=1,\n",
    "                 shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=1,\n",
    "                shuffle=False)\n",
    "\n",
    "print (\"Number of training examples: \" + str(train_set.__len__()))\n",
    "print (\"Number of testing examples: \" + str(test_set.__len__()))\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "seq, seq_target = dataiter.next()\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    npimg = (npimg * 255).transpose(1,2,0).astype(np.uint8)\n",
    "    plt.figure(figsize = (20,20))\n",
    "    plt.imshow(npimg, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "# show images\n",
    "grid = torchvision.utils.make_grid(torch.cat((seq[0], seq_target[0]), 0), nrow=20, padding=0)\n",
    "imshow(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97095b9e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab1737ea95185935ab2aa12c82e07071",
     "grade": false,
     "grade_id": "cell-e418102e76576911",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3. Convolutional LSTM Cell\n",
    "\n",
    "In the class, you have learned the idea of of vanilla Long Short-Term Memory networks (LSTM), where the detailed equations are given as follows:\n",
    "\n",
    "<br/>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_t &= \\sigma_g(W_{f} \\cdot [x_t, h_{t-1}] + b_f) \\\\\n",
    "i_t &= \\sigma_g(W_{i} \\cdot [x_t, h_{t-1}] + b_i) \\\\\n",
    "c_t &= f_t \\circ c_{t-1} + i_t \\circ \\sigma_c(W_{c} \\cdot [x_t, h_{t-1}] + b_c) \\\\\n",
    "o_t &= \\sigma_g(W_{o} \\cdot [x_t, h_{t-1}] + b_o) \\\\\n",
    "h_t &= o_t \\circ \\sigma_h(c_t)\n",
    "\\end{align}\n",
    "$$\n",
    "<br/>\n",
    "\n",
    "Compared to the LSTM Cell covered in the class, here you are required to implement a variant of it -- ConvLSTM. This variant modifies the inner workings of the LSTM mechanism by using the convolution operation instead of simple matrix multiplication (Note: original ConvLSTM cell design may use the cell state in all the gates, here we are actually implementing a variant of it). The equations are as follows:\n",
    "\n",
    "<br/>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_t &= \\sigma_g(W_{f} * [x_t, h_{t-1}] + b_f) \\\\\n",
    "i_t &= \\sigma_g(W_{i} * [x_t, h_{t-1}] + b_i) \\\\\n",
    "c_t &= f_t \\circ c_{t-1} + i_t \\circ \\sigma_c(W_{c} * [x_t, h_{t-1}] + b_c) \\\\\n",
    "o_t &= \\sigma_g(W_{o} * [x_t, h_{t-1}] + b_o) \\\\\n",
    "h_t &= o_t \\circ \\sigma_h(c_t)\n",
    "\\end{align}\n",
    "$$\n",
    "<br/>\n",
    "\n",
    "where $*$ denotes convolutional operation and $\\circ$ denotes Hadamard product (element-wise product). The initial values are $c_0 = 0$ and $h_0 = 0$. The subscript $t$ indexes the time step. See the figure below as a reference: \n",
    "\n",
    "<img src=\"images/convlstm1.png\" width=\"60%\">\n",
    "\n",
    "**Variables**\n",
    "\n",
    "* $x_t \\in \\mathbb{R}^{d}$: input vector to the ConvLSTM unit\n",
    "* $f_t \\in {(0,1)}^{h}$: forget gate's activation vector\n",
    "* $i_t \\in {(0,1)}^{h}$: input/update gate's activation vector\n",
    "* $o_t \\in {(0,1)}^{h}$: output gate's activation vector\n",
    "* $h_t \\in {(-1,1)}^{h}$: hidden state vector also known as output vector of the LSTM unit\n",
    "* $c_t \\in \\mathbb{R}^{h}$: cell state vector\n",
    "* $W \\in \\mathbb{R}^{h \\times d}$, $ \\in \\mathbb{R}^{h \\times h} $ and $b \\in \\mathbb{R}^{h}$: weight matrices and bias vector parameters which need to be learned during training\n",
    "\n",
    "where the superscripts $d$ and $h$ refer to the number of input features and number of hidden units, respectively.\n",
    "\n",
    "**Activation function**\n",
    "* $\\sigma_g$: sigmoid function.\n",
    "* $\\sigma_c$: hyperbolic tangent function.\n",
    "* $\\sigma_h$: hyperbolic tangent function.\n",
    "\n",
    "**Why ConvLSTM here?**\n",
    "For general-purpose sequence modeling, LSTM as a special RNN structure has proven stable and powerful for modeling long-range dependencies. In this kind of RNN structure, the previous hidden state is passed to the next step of sequence. Therefore, information on previous sequence data can be held and then used to predict the future. In our case of video prediction, LSTM is adopted to hold information on the past frames, and predict the future digit movements based on the previous observation.\n",
    "\n",
    "Although the vanilla LSTM cell has proven powerful for handling temporal correlation, it uses full connections in input-to-state and state-to-state transitions, in which no spatial information is encoded. To address this, we use a convolutional operator in the input-to-state and state-to-state transitions, such that the ConvLSTM cell can determine the future state of a certain cell in the grid by the inputs and past states of its local neighbors. With ConvLSTM, the motion of moving digits can be captured.\n",
    "\n",
    "---\n",
    "\n",
    "**Question:** Given the above information, now you need to implement the ``ConvLSTM`` cell (``nn.lstm()`` is not allowed to call in your implementation):\n",
    "\n",
    "\n",
    "1.   Initialize the parameters in ConvLSTM Cell. You can call ``nn.Conv2d()`` to implment the convolution.\n",
    "2.   Given the current states, compute and update it to get the next states (based on the equations above).\n",
    "3.   Complete the state initialization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aedbc9ee",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7033949cffbd6c5c71041c5e3e8f7b30",
     "grade": true,
     "grade_id": "cell-fb38fc30e49c55c7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "\n",
    "        Arguments\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = int((kernel_size[0] - 1) / 2)\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.w_c = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim, out_channels=self.hidden_dim, kernel_size=self.kernel_size, padding=self.padding, bias=self.bias)\n",
    "        self.w_i = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim, out_channels=self.hidden_dim, kernel_size=self.kernel_size, padding=self.padding, bias=self.bias)\n",
    "        self.w_f = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim, out_channels=self.hidden_dim, kernel_size=self.kernel_size, padding=self.padding, bias=self.bias)\n",
    "        self.w_o = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim, out_channels=self.hidden_dim, kernel_size=self.kernel_size, padding=self.padding, bias=self.bias)\n",
    "\n",
    "    def forward(self, input, cur_state): # given the input and current states, you need to get the next states\n",
    "        \"\"\"\n",
    "        Compute and update the states.\n",
    "\n",
    "        Arguments\n",
    "        ----------\n",
    "        input:\n",
    "            4-D Tensor of shape (b, c, h, w)        #   batch, channel, height, width\n",
    "        cur_state: \n",
    "            [h_current, c_current] input to current cell, including hidden state and cell input \n",
    "        \n",
    "        \n",
    "        ----------\n",
    "        Output: \n",
    "            [h_next, c_next] input to next cell, including update hidden state and cell output \n",
    "        \"\"\"     \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        h_cur = cur_state[0]\n",
    "        c_cur = cur_state[-1]\n",
    "        \n",
    "        h_cur = h_cur.to(\"cuda\")\n",
    "        c_cur = c_cur.to(\"cuda\")\n",
    "        #input = input.to(\"cuda\")\n",
    "        x_h = torch.cat([input, h_cur], dim=1)\n",
    "        \n",
    "        ft = torch.sigmoid(self.w_f(x_h))\n",
    "        it = torch.sigmoid(self.w_i(x_h))\n",
    "        ct = ft * c_cur + it * torch.tanh(self.w_c(x_h))\n",
    "        ot = torch.sigmoid(self.w_o(x_h))\n",
    "        ht = ot * torch.tanh(ct)\n",
    "\n",
    "        return  ht,  ct\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        \"\"\"\n",
    "        Initialize the input states.\n",
    "\n",
    "        Arguments\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            Number of examples in a batch.\n",
    "        image_size: (int, int)\n",
    "            The spatial resolution of input images.\n",
    "            \n",
    "        ----------\n",
    "        Output: \n",
    "            [h_init, c_init] zero tensors\n",
    "        \n",
    "        Note: the initialized states should also be on the gpu machine\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        height, width = image_size\n",
    "        \n",
    "        h_init = torch.zeros(batch_size, self.hidden_dim, height, width)\n",
    "        c_init = torch.zeros(batch_size, self.hidden_dim, height, width)\n",
    "        \n",
    "        return (h_init, c_init)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304af17",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22886e053be89239e4153b2e8b1d1087",
     "grade": false,
     "grade_id": "cell-590ec83714d933f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4. Encoder-Decoder Sequnce-to-Sequence Architecture\n",
    "\n",
    "We follow the design patterns of RNNs that map a variable-length sequence to another variable-lenght sequence (in the class slides). An encoder that contains two ConvLSTM cells extract the \"context\" that represents the motion summary of the input sequential images and is given as input to the decoder RNN, which also contains two ConvLSTM cells. The network architecutre is as follows:\n",
    "\n",
    "<br/>\n",
    "\n",
    "Input ---> ConvLSTM 1 ---> ConvLSTM 2 ---> Encoded State ---> ConvLSTM 3 ---> ConvLSTM 4 ---> 3D CNN decoder ---> Output\n",
    "\n",
    "<br/>\n",
    "\n",
    "* **Input:** In the dataset, the size of input image is a tensor $ X \\in \\mathbb{R}^{B \\times 1 \\times 64 \\times 64}$, where $B$ denotes the batch size. Generally, the input shape of a particular time step is ``[batch_size, num_channel, height, width]`` . ``Width`` and ``height`` are the image size, time step means which frame the network is processing.\n",
    "\n",
    "\n",
    "* **Encoder**\n",
    "  \n",
    "  * ConvLSTM Encoder 1: the output shape is ``[batch_size, num_hidden_dim,  height, width]`` , where ``num_hidden_dim`` is the output dimension of parameters.\n",
    "  * ConvLSTM Encoder 2: the output shape is ``[batch_size, num_hidden_dim, height, width]`` .\n",
    "\n",
    "\n",
    "* **Decoder**\n",
    "  * ConvLSTM Decoder 3: the output shape is ``[batch_size, num_hidden_dim,  height, width]`` .\n",
    "  * ConvLSTM Decoder 4: the output shape is ``[batch_size, num_hidden_dim,  height, width]`` .\n",
    "\n",
    "\n",
    "* **3D CNN Decoder:**\n",
    "Since we want to regress images from the hidden states, we need to transform the feature maps from decoder into actual predictions. \n",
    "To achieve this we implement a 3D-CNN layer: 1) Take as input ``[num_hidden_dim, height, width]`` for each example in batch and time step 2) Iterate over the $n$ hidden states 3) Output ``[num_channel, height, width]`` per iteration, which is the predicted frame.\n",
    "\n",
    "  * Sigmoid layer: For the activation layer, as we normalized images into [0,1] at the beginning, we use a sigmoid actvation function to transform the output values of 3D-CNN layer into [0,1] as well. \n",
    "\n",
    "**How to predict the next $n$ frames?**\n",
    "\n",
    "Rather than predict all future frames in one-go, you will feed the \"context\" obtained from encoder into ConvLSTM decoder to get a final hidden state at one time. This final hidden state will then go through the 3D CNN decoder to get an actual predicted frame. For the first predicted frame, the output from the encoder can be used as \"context\". For the $n$-th predicted frame ($n>1$), the final hidden state of ($n$-1)-th frame can be used as \"context\" to input to the ConvLSTM decoder. By iterating $n$ times, the network can predict the next $n$ frames.\n",
    "\n",
    "In this case, you can produce any number of predictions in the future without having to change the architecture.\n",
    "\n",
    "---\n",
    "\n",
    "**Question:** Given the above information, you will need to build the encoder-decoder network:\n",
    "\n",
    "\n",
    "1.   Initialize the parameters in ``EncoderDecoderConvLSTM``. You can call ``nn.Conv3d()`` to implment the 3D CNN layer and use the above ConvLSTM cell to build the encoder and decoder.\n",
    "2.   Define the encoder->decoder forward pass in ``autoencder()``.\n",
    "3.   Complete the forward pass of the whole network: first initialize the states and then gradually update them. (Hint: you can call ``init_hidden()`` function in ConvLSTM cell for state initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32e73eec",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "821b917b4601da505a3bc2d35ecaabd1",
     "grade": true,
     "grade_id": "cell-073d7e45da0f13f8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderConvLSTM(nn.Module):\n",
    "    def __init__(self, num_hidden_dim, in_channel):\n",
    "        super(EncoderDecoderConvLSTM, self).__init__()\n",
    "\n",
    "        \"\"\" ARCHITECTURE \n",
    "\n",
    "        # Encoder (ConvLSTM)\n",
    "        # Encoded Vector (final hidden state of encoder)\n",
    "        # Decoder (ConvLSTM) - takes Encoded Vector as input\n",
    "        # Decoder (3D CNN) - regresses frames from hidden states\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        self.num_hidden_dim = num_hidden_dim\n",
    "        self.in_channel = in_channel\n",
    "        \n",
    "        self.encoder_1 = ConvLSTMCell(self.in_channel, self.num_hidden_dim, kernel_size=(3,3), bias=True)\n",
    "        self.encoder_2 = ConvLSTMCell(self.num_hidden_dim, self.num_hidden_dim, kernel_size=(3,3), bias=True)\n",
    "        self.decoder_3 = ConvLSTMCell(self.num_hidden_dim, self.num_hidden_dim, kernel_size=(3,3), bias=True)\n",
    "        self.decoder_4 = ConvLSTMCell(self.num_hidden_dim, self.num_hidden_dim, kernel_size=(3,3), bias=True)\n",
    "        \n",
    "        self.decoder_CNN = nn.Conv3d(in_channels=self.num_hidden_dim, out_channels=self.in_channel, kernel_size=3, padding=1, bias=True)\n",
    "\n",
    "\n",
    "    def autoencoder(self, x, seq_len, future_step, States):\n",
    "        \"\"\"\n",
    "        Build the encoder-decoder forward pass\n",
    "\n",
    "        Arguments\n",
    "        ----------\n",
    "        x: \n",
    "            5-D Tensor of shape (b, t, c, h, w)       #   batch_size, time_step, num_channel, height, width\n",
    "        seq_len: int\n",
    "            Length of input sequence.\n",
    "        future_step: int\n",
    "            Length of predicted sequence.\n",
    "        States: bool\n",
    "            Initialized states for each cell.\n",
    "        \"\"\"\n",
    "        \n",
    "        # List to store the final hidden state of all the predicted frames \n",
    "        outputs = []\n",
    "        \n",
    "        # The initialized state vectors\n",
    "        h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4 = States\n",
    "        \n",
    "        \n",
    "        # encoder\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            h_t, c_t = self.encoder_1(x[:, t, :, :, :], [h_t, c_t])\n",
    "            h_t2, c_t2 = self.encoder_2(h_t, [h_t2, c_t2])\n",
    "        \n",
    "        # encoded_vector: \"context\" from the encoder\n",
    "        # YOUR CODE HERE\n",
    "        encoded_vector = h_t2\n",
    "\n",
    "        # decoder: you should store the final hidden states h_t4 from decoder in list outputs\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        for t in range(future_step):\n",
    "            h_t3, c_t3 = self.decoder_3(encoded_vector, [h_t3, c_t3])\n",
    "            h_t4, c_t4 = self.decoder_4(h_t3, [h_t4, c_t4])\n",
    "            encoded_vector = h_t4\n",
    "            outputs.append(h_t4)\n",
    "\n",
    "        # regression\n",
    "        outputs = torch.stack(outputs, 1)\n",
    "        outputs = outputs.permute(0, 2, 1, 3, 4)\n",
    "        outputs = self.decoder_CNN(outputs)\n",
    "        predicted_frames = torch.nn.Sigmoid()(outputs)\n",
    "\n",
    "        return predicted_frames\n",
    "\n",
    "    def forward(self, x, future_seq=0):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x:\n",
    "            5-D Tensor of shape (b, t, c, h, w)        #   batch, time, num_channel, height, width\n",
    "        future_seq:\n",
    "            Length of predicted sequence\n",
    "        \"\"\"\n",
    "\n",
    "        # find size of different input dimensions\n",
    "        b, seq_len, _, h, w = x.size()\n",
    "        \n",
    "        # initialize hidden states\n",
    "        # YOUR CODE HERE\n",
    "        h_t, c_t = self.encoder_1.init_hidden(b, (h, w))\n",
    "        h_t2, c_t2 = self.encoder_2.init_hidden(b, (h, w))\n",
    "        h_t3, c_t3 = self.decoder_3.init_hidden(b, (h, w))\n",
    "        h_t4, c_t4 = self.decoder_4.init_hidden(b, (h, w))\n",
    "        \n",
    "        States = [h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4]\n",
    "        \n",
    "        # autoencoder forward\n",
    "        # YOUR CODE HERE\n",
    "        predicted_frames = self.autoencoder(x, seq_len, future_seq, States)\n",
    "        \n",
    "        return predicted_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb58e5aa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7ffb3ab8b1646fd2ad753ffe3770f08",
     "grade": false,
     "grade_id": "cell-e03091d5aa5f2866",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5. Training and Evaluation\n",
    "\n",
    "We provide you helper functions for training and evaluation:\n",
    "\n",
    "```python\n",
    "    def save_img(img, args):\n",
    "        \"\"\"\n",
    "        TODO: Save images in your checkpoint directory\n",
    "        \"\"\"\n",
    "    \n",
    "    def show_video(x, y_hat, y):\n",
    "        \"\"\"\n",
    "        TODO: Make the predicted frames and ground truth frames as a image\n",
    "        \"\"\"\n",
    "\n",
    "    def train(net, loader, criterion, optimizer, epoch, args):\n",
    "        \"\"\"\n",
    "        TODO: Training function for each epoch\n",
    "        \"\"\"\n",
    "\n",
    "    def evaluate_epoch(net, loader, criterion, epoch, args):\n",
    "        \"\"\"\n",
    "        TODO: Evaluation function for each epoch\n",
    "        \"\"\"    \n",
    "        \n",
    "    def evaluate(net, loader, criterion, args):\n",
    "        \"\"\"\n",
    "        TODO: Evaluation function for the whole test set\n",
    "        \"\"\"\n",
    "\n",
    "    def create_optimizer(net, learning_rate):\n",
    "        \"\"\"\n",
    "        TODO: Create optimizer for updating network parameters\n",
    "        \"\"\"\n",
    "\n",
    "    def create_criterion():\n",
    "        \"\"\"\n",
    "        TODO: Create criterion for gradients backward\n",
    "        \"\"\"\n",
    "    def checkpoint(net, epoch, cur_loss, args):\n",
    "        \"\"\"\n",
    "        TODO: Save your trained model as file\n",
    "        \"\"\"\n",
    "```\n",
    "\n",
    "Here are some examples of output, you are expected to get similar results or even better! (Top row contains the input sequence and the predicted frames, while the second row represents the input sequence and ground truth)\n",
    "\n",
    "<img src=\"./images/result50.jpg\" width=\"90%\">\n",
    "<img src=\"./images/result55.jpg\" width=\"90%\">\n",
    "<img src=\"./images/result62.jpg\" width=\"90%\">\n",
    "\n",
    "The Average Mean-Square Error (MSE) on the test set of a model trained with 100 epochs is around **0.0280**. \n",
    "\n",
    "**Question**: You can found that the results are not perfect, please write a paragraph in the report to explain why the generated frames are \"fading away\". \n",
    "\n",
    "**Bonus (10%)**: Based on the above conclusion, you could think of ways to improve your code, and explain your observations and method in the report.\n",
    "\n",
    "---\n",
    "\n",
    "We use ``argparse`` to manage the experimental setting, e.g., hyperparameters and data directory. You can follow the default setting or change it with your choices.\n",
    "\n",
    "If \"out-of-memory\" error message occurs, you may try to reduce the batch size or hidden_dims. \n",
    "\n",
    "When the training is done, you can set ``args.load=True`` and ``args.eval=True`` to get both qualititative and quantitative analysis of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3340372",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed6197c3243f1ac14d4d9072f2637c44",
     "grade": true,
     "grade_id": "cell-1b7a844fe3c818d1",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/100], Step: [0] learning_rate: 0.0001, loss: 0.2390\n",
      "Epoch:[1/100], Step: [50] learning_rate: 0.0001, loss: 0.0478\n",
      "Epoch:[1/100], Step: [100] learning_rate: 0.0001, loss: 0.0391\n",
      "Epoch:[1/100], Step: [150] learning_rate: 0.0001, loss: 0.0421\n",
      "Epoch:[1/100], Step: [200] learning_rate: 0.0001, loss: 0.0447\n",
      "Epoch:[1/100], Step: [250] learning_rate: 0.0001, loss: 0.0379\n",
      "Epoch:[1/100], Step: [300] learning_rate: 0.0001, loss: 0.0398\n",
      "Epoch:[1/100], Step: [350] learning_rate: 0.0001, loss: 0.0426\n",
      "Epoch:[1/100], Step: [400] learning_rate: 0.0001, loss: 0.0372\n",
      "Epoch:[1/100], Step: [450] learning_rate: 0.0001, loss: 0.0398\n",
      "Epoch:[1/100], Step: [500] learning_rate: 0.0001, loss: 0.0375\n",
      "Epoch:[1/100], Step: [550] learning_rate: 0.0001, loss: 0.0372\n",
      "Epoch:[1/100], Step: [600] learning_rate: 0.0001, loss: 0.0383\n",
      "Average MSE loss on test dataset: 0.0396\n",
      "Saving checkpoints at 1 epochs.\n",
      "Epoch:[2/100], Step: [0] learning_rate: 0.0001, loss: 0.0390\n",
      "Epoch:[2/100], Step: [50] learning_rate: 0.0001, loss: 0.0382\n",
      "Epoch:[2/100], Step: [100] learning_rate: 0.0001, loss: 0.0362\n",
      "Epoch:[2/100], Step: [150] learning_rate: 0.0001, loss: 0.0410\n",
      "Epoch:[2/100], Step: [200] learning_rate: 0.0001, loss: 0.0360\n",
      "Epoch:[2/100], Step: [250] learning_rate: 0.0001, loss: 0.0377\n",
      "Epoch:[2/100], Step: [300] learning_rate: 0.0001, loss: 0.0361\n",
      "Epoch:[2/100], Step: [350] learning_rate: 0.0001, loss: 0.0390\n",
      "Epoch:[2/100], Step: [400] learning_rate: 0.0001, loss: 0.0372\n",
      "Epoch:[2/100], Step: [450] learning_rate: 0.0001, loss: 0.0396\n",
      "Epoch:[2/100], Step: [500] learning_rate: 0.0001, loss: 0.0409\n",
      "Epoch:[2/100], Step: [550] learning_rate: 0.0001, loss: 0.0376\n",
      "Epoch:[2/100], Step: [600] learning_rate: 0.0001, loss: 0.0386\n",
      "Average MSE loss on test dataset: 0.0387\n",
      "Saving checkpoints at 2 epochs.\n",
      "Epoch:[3/100], Step: [0] learning_rate: 0.0001, loss: 0.0378\n",
      "Epoch:[3/100], Step: [50] learning_rate: 0.0001, loss: 0.0339\n",
      "Epoch:[3/100], Step: [100] learning_rate: 0.0001, loss: 0.0399\n",
      "Epoch:[3/100], Step: [150] learning_rate: 0.0001, loss: 0.0396\n",
      "Epoch:[3/100], Step: [200] learning_rate: 0.0001, loss: 0.0406\n",
      "Epoch:[3/100], Step: [250] learning_rate: 0.0001, loss: 0.0345\n",
      "Epoch:[3/100], Step: [300] learning_rate: 0.0001, loss: 0.0402\n",
      "Epoch:[3/100], Step: [350] learning_rate: 0.0001, loss: 0.0379\n",
      "Epoch:[3/100], Step: [400] learning_rate: 0.0001, loss: 0.0398\n",
      "Epoch:[3/100], Step: [450] learning_rate: 0.0001, loss: 0.0353\n",
      "Epoch:[3/100], Step: [500] learning_rate: 0.0001, loss: 0.0343\n",
      "Epoch:[3/100], Step: [550] learning_rate: 0.0001, loss: 0.0368\n",
      "Epoch:[3/100], Step: [600] learning_rate: 0.0001, loss: 0.0352\n",
      "Average MSE loss on test dataset: 0.0369\n",
      "Saving checkpoints at 3 epochs.\n",
      "Epoch:[4/100], Step: [0] learning_rate: 0.0001, loss: 0.0350\n",
      "Epoch:[4/100], Step: [50] learning_rate: 0.0001, loss: 0.0370\n",
      "Epoch:[4/100], Step: [100] learning_rate: 0.0001, loss: 0.0371\n",
      "Epoch:[4/100], Step: [150] learning_rate: 0.0001, loss: 0.0341\n",
      "Epoch:[4/100], Step: [200] learning_rate: 0.0001, loss: 0.0370\n",
      "Epoch:[4/100], Step: [250] learning_rate: 0.0001, loss: 0.0335\n",
      "Epoch:[4/100], Step: [300] learning_rate: 0.0001, loss: 0.0389\n",
      "Epoch:[4/100], Step: [350] learning_rate: 0.0001, loss: 0.0367\n",
      "Epoch:[4/100], Step: [400] learning_rate: 0.0001, loss: 0.0330\n",
      "Epoch:[4/100], Step: [450] learning_rate: 0.0001, loss: 0.0343\n",
      "Epoch:[4/100], Step: [500] learning_rate: 0.0001, loss: 0.0349\n",
      "Epoch:[4/100], Step: [550] learning_rate: 0.0001, loss: 0.0331\n",
      "Epoch:[4/100], Step: [600] learning_rate: 0.0001, loss: 0.0375\n",
      "Average MSE loss on test dataset: 0.0361\n",
      "Saving checkpoints at 4 epochs.\n",
      "Epoch:[5/100], Step: [0] learning_rate: 0.0001, loss: 0.0362\n",
      "Epoch:[5/100], Step: [50] learning_rate: 0.0001, loss: 0.0356\n",
      "Epoch:[5/100], Step: [100] learning_rate: 0.0001, loss: 0.0387\n",
      "Epoch:[5/100], Step: [150] learning_rate: 0.0001, loss: 0.0348\n",
      "Epoch:[5/100], Step: [200] learning_rate: 0.0001, loss: 0.0344\n",
      "Epoch:[5/100], Step: [250] learning_rate: 0.0001, loss: 0.0385\n",
      "Epoch:[5/100], Step: [300] learning_rate: 0.0001, loss: 0.0351\n",
      "Epoch:[5/100], Step: [350] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[5/100], Step: [400] learning_rate: 0.0001, loss: 0.0355\n",
      "Epoch:[5/100], Step: [450] learning_rate: 0.0001, loss: 0.0366\n",
      "Epoch:[5/100], Step: [500] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[5/100], Step: [550] learning_rate: 0.0001, loss: 0.0388\n",
      "Epoch:[5/100], Step: [600] learning_rate: 0.0001, loss: 0.0343\n",
      "Average MSE loss on test dataset: 0.0356\n",
      "Saving checkpoints at 5 epochs.\n",
      "Epoch:[6/100], Step: [0] learning_rate: 0.0001, loss: 0.0348\n",
      "Epoch:[6/100], Step: [50] learning_rate: 0.0001, loss: 0.0349\n",
      "Epoch:[6/100], Step: [100] learning_rate: 0.0001, loss: 0.0341\n",
      "Epoch:[6/100], Step: [150] learning_rate: 0.0001, loss: 0.0360\n",
      "Epoch:[6/100], Step: [200] learning_rate: 0.0001, loss: 0.0332\n",
      "Epoch:[6/100], Step: [250] learning_rate: 0.0001, loss: 0.0346\n",
      "Epoch:[6/100], Step: [300] learning_rate: 0.0001, loss: 0.0387\n",
      "Epoch:[6/100], Step: [350] learning_rate: 0.0001, loss: 0.0378\n",
      "Epoch:[6/100], Step: [400] learning_rate: 0.0001, loss: 0.0393\n",
      "Epoch:[6/100], Step: [450] learning_rate: 0.0001, loss: 0.0347\n",
      "Epoch:[6/100], Step: [500] learning_rate: 0.0001, loss: 0.0381\n",
      "Epoch:[6/100], Step: [550] learning_rate: 0.0001, loss: 0.0350\n",
      "Epoch:[6/100], Step: [600] learning_rate: 0.0001, loss: 0.0317\n",
      "Average MSE loss on test dataset: 0.0351\n",
      "Saving checkpoints at 6 epochs.\n",
      "Epoch:[7/100], Step: [0] learning_rate: 0.0001, loss: 0.0354\n",
      "Epoch:[7/100], Step: [50] learning_rate: 0.0001, loss: 0.0369\n",
      "Epoch:[7/100], Step: [100] learning_rate: 0.0001, loss: 0.0328\n",
      "Epoch:[7/100], Step: [150] learning_rate: 0.0001, loss: 0.0337\n",
      "Epoch:[7/100], Step: [200] learning_rate: 0.0001, loss: 0.0343\n",
      "Epoch:[7/100], Step: [250] learning_rate: 0.0001, loss: 0.0355\n",
      "Epoch:[7/100], Step: [300] learning_rate: 0.0001, loss: 0.0345\n",
      "Epoch:[7/100], Step: [350] learning_rate: 0.0001, loss: 0.0341\n",
      "Epoch:[7/100], Step: [400] learning_rate: 0.0001, loss: 0.0378\n",
      "Epoch:[7/100], Step: [450] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[7/100], Step: [500] learning_rate: 0.0001, loss: 0.0349\n",
      "Epoch:[7/100], Step: [550] learning_rate: 0.0001, loss: 0.0338\n",
      "Epoch:[7/100], Step: [600] learning_rate: 0.0001, loss: 0.0368\n",
      "Average MSE loss on test dataset: 0.0348\n",
      "Saving checkpoints at 7 epochs.\n",
      "Epoch:[8/100], Step: [0] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[8/100], Step: [50] learning_rate: 0.0001, loss: 0.0346\n",
      "Epoch:[8/100], Step: [100] learning_rate: 0.0001, loss: 0.0352\n",
      "Epoch:[8/100], Step: [150] learning_rate: 0.0001, loss: 0.0362\n",
      "Epoch:[8/100], Step: [200] learning_rate: 0.0001, loss: 0.0328\n",
      "Epoch:[8/100], Step: [250] learning_rate: 0.0001, loss: 0.0335\n",
      "Epoch:[8/100], Step: [300] learning_rate: 0.0001, loss: 0.0340\n",
      "Epoch:[8/100], Step: [350] learning_rate: 0.0001, loss: 0.0339\n",
      "Epoch:[8/100], Step: [400] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[8/100], Step: [450] learning_rate: 0.0001, loss: 0.0335\n",
      "Epoch:[8/100], Step: [500] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[8/100], Step: [550] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[8/100], Step: [600] learning_rate: 0.0001, loss: 0.0336\n",
      "Average MSE loss on test dataset: 0.0343\n",
      "Saving checkpoints at 8 epochs.\n",
      "Epoch:[9/100], Step: [0] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[9/100], Step: [50] learning_rate: 0.0001, loss: 0.0333\n",
      "Epoch:[9/100], Step: [100] learning_rate: 0.0001, loss: 0.0342\n",
      "Epoch:[9/100], Step: [150] learning_rate: 0.0001, loss: 0.0355\n",
      "Epoch:[9/100], Step: [200] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[9/100], Step: [250] learning_rate: 0.0001, loss: 0.0341\n",
      "Epoch:[9/100], Step: [300] learning_rate: 0.0001, loss: 0.0346\n",
      "Epoch:[9/100], Step: [350] learning_rate: 0.0001, loss: 0.0327\n",
      "Epoch:[9/100], Step: [400] learning_rate: 0.0001, loss: 0.0339\n",
      "Epoch:[9/100], Step: [450] learning_rate: 0.0001, loss: 0.0333\n",
      "Epoch:[9/100], Step: [500] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[9/100], Step: [550] learning_rate: 0.0001, loss: 0.0324\n",
      "Epoch:[9/100], Step: [600] learning_rate: 0.0001, loss: 0.0344\n",
      "Average MSE loss on test dataset: 0.0338\n",
      "Saving checkpoints at 9 epochs.\n",
      "Epoch:[10/100], Step: [0] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[10/100], Step: [50] learning_rate: 0.0001, loss: 0.0335\n",
      "Epoch:[10/100], Step: [100] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[10/100], Step: [150] learning_rate: 0.0001, loss: 0.0391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[10/100], Step: [200] learning_rate: 0.0001, loss: 0.0353\n",
      "Epoch:[10/100], Step: [250] learning_rate: 0.0001, loss: 0.0333\n",
      "Epoch:[10/100], Step: [300] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[10/100], Step: [350] learning_rate: 0.0001, loss: 0.0340\n",
      "Epoch:[10/100], Step: [400] learning_rate: 0.0001, loss: 0.0346\n",
      "Epoch:[10/100], Step: [450] learning_rate: 0.0001, loss: 0.0340\n",
      "Epoch:[10/100], Step: [500] learning_rate: 0.0001, loss: 0.0327\n",
      "Epoch:[10/100], Step: [550] learning_rate: 0.0001, loss: 0.0331\n",
      "Epoch:[10/100], Step: [600] learning_rate: 0.0001, loss: 0.0337\n",
      "Average MSE loss on test dataset: 0.0335\n",
      "Saving checkpoints at 10 epochs.\n",
      "Epoch:[11/100], Step: [0] learning_rate: 0.0001, loss: 0.0349\n",
      "Epoch:[11/100], Step: [50] learning_rate: 0.0001, loss: 0.0354\n",
      "Epoch:[11/100], Step: [100] learning_rate: 0.0001, loss: 0.0345\n",
      "Epoch:[11/100], Step: [150] learning_rate: 0.0001, loss: 0.0334\n",
      "Epoch:[11/100], Step: [200] learning_rate: 0.0001, loss: 0.0329\n",
      "Epoch:[11/100], Step: [250] learning_rate: 0.0001, loss: 0.0336\n",
      "Epoch:[11/100], Step: [300] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[11/100], Step: [350] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[11/100], Step: [400] learning_rate: 0.0001, loss: 0.0332\n",
      "Epoch:[11/100], Step: [450] learning_rate: 0.0001, loss: 0.0324\n",
      "Epoch:[11/100], Step: [500] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[11/100], Step: [550] learning_rate: 0.0001, loss: 0.0353\n",
      "Epoch:[11/100], Step: [600] learning_rate: 0.0001, loss: 0.0310\n",
      "Average MSE loss on test dataset: 0.0332\n",
      "Saving checkpoints at 11 epochs.\n",
      "Epoch:[12/100], Step: [0] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[12/100], Step: [50] learning_rate: 0.0001, loss: 0.0343\n",
      "Epoch:[12/100], Step: [100] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[12/100], Step: [150] learning_rate: 0.0001, loss: 0.0330\n",
      "Epoch:[12/100], Step: [200] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[12/100], Step: [250] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[12/100], Step: [300] learning_rate: 0.0001, loss: 0.0319\n",
      "Epoch:[12/100], Step: [350] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[12/100], Step: [400] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[12/100], Step: [450] learning_rate: 0.0001, loss: 0.0319\n",
      "Epoch:[12/100], Step: [500] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[12/100], Step: [550] learning_rate: 0.0001, loss: 0.0347\n",
      "Epoch:[12/100], Step: [600] learning_rate: 0.0001, loss: 0.0312\n",
      "Average MSE loss on test dataset: 0.0330\n",
      "Saving checkpoints at 12 epochs.\n",
      "Epoch:[13/100], Step: [0] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[13/100], Step: [50] learning_rate: 0.0001, loss: 0.0319\n",
      "Epoch:[13/100], Step: [100] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[13/100], Step: [150] learning_rate: 0.0001, loss: 0.0329\n",
      "Epoch:[13/100], Step: [200] learning_rate: 0.0001, loss: 0.0314\n",
      "Epoch:[13/100], Step: [250] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[13/100], Step: [300] learning_rate: 0.0001, loss: 0.0335\n",
      "Epoch:[13/100], Step: [350] learning_rate: 0.0001, loss: 0.0329\n",
      "Epoch:[13/100], Step: [400] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[13/100], Step: [450] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[13/100], Step: [500] learning_rate: 0.0001, loss: 0.0332\n",
      "Epoch:[13/100], Step: [550] learning_rate: 0.0001, loss: 0.0335\n",
      "Epoch:[13/100], Step: [600] learning_rate: 0.0001, loss: 0.0304\n",
      "Average MSE loss on test dataset: 0.0328\n",
      "Saving checkpoints at 13 epochs.\n",
      "Epoch:[14/100], Step: [0] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[14/100], Step: [50] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[14/100], Step: [100] learning_rate: 0.0001, loss: 0.0364\n",
      "Epoch:[14/100], Step: [150] learning_rate: 0.0001, loss: 0.0354\n",
      "Epoch:[14/100], Step: [200] learning_rate: 0.0001, loss: 0.0341\n",
      "Epoch:[14/100], Step: [250] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[14/100], Step: [300] learning_rate: 0.0001, loss: 0.0344\n",
      "Epoch:[14/100], Step: [350] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[14/100], Step: [400] learning_rate: 0.0001, loss: 0.0327\n",
      "Epoch:[14/100], Step: [450] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[14/100], Step: [500] learning_rate: 0.0001, loss: 0.0346\n",
      "Epoch:[14/100], Step: [550] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[14/100], Step: [600] learning_rate: 0.0001, loss: 0.0288\n",
      "Average MSE loss on test dataset: 0.0325\n",
      "Saving checkpoints at 14 epochs.\n",
      "Epoch:[15/100], Step: [0] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[15/100], Step: [50] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[15/100], Step: [100] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[15/100], Step: [150] learning_rate: 0.0001, loss: 0.0344\n",
      "Epoch:[15/100], Step: [200] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[15/100], Step: [250] learning_rate: 0.0001, loss: 0.0336\n",
      "Epoch:[15/100], Step: [300] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[15/100], Step: [350] learning_rate: 0.0001, loss: 0.0343\n",
      "Epoch:[15/100], Step: [400] learning_rate: 0.0001, loss: 0.0356\n",
      "Epoch:[15/100], Step: [450] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[15/100], Step: [500] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[15/100], Step: [550] learning_rate: 0.0001, loss: 0.0339\n",
      "Epoch:[15/100], Step: [600] learning_rate: 0.0001, loss: 0.0303\n",
      "Average MSE loss on test dataset: 0.0325\n",
      "Saving checkpoints at 15 epochs.\n",
      "Epoch:[16/100], Step: [0] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[16/100], Step: [50] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[16/100], Step: [100] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[16/100], Step: [150] learning_rate: 0.0001, loss: 0.0347\n",
      "Epoch:[16/100], Step: [200] learning_rate: 0.0001, loss: 0.0277\n",
      "Epoch:[16/100], Step: [250] learning_rate: 0.0001, loss: 0.0357\n",
      "Epoch:[16/100], Step: [300] learning_rate: 0.0001, loss: 0.0335\n",
      "Epoch:[16/100], Step: [350] learning_rate: 0.0001, loss: 0.0349\n",
      "Epoch:[16/100], Step: [400] learning_rate: 0.0001, loss: 0.0342\n",
      "Epoch:[16/100], Step: [450] learning_rate: 0.0001, loss: 0.0341\n",
      "Epoch:[16/100], Step: [500] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[16/100], Step: [550] learning_rate: 0.0001, loss: 0.0342\n",
      "Epoch:[16/100], Step: [600] learning_rate: 0.0001, loss: 0.0317\n",
      "Average MSE loss on test dataset: 0.0322\n",
      "Saving checkpoints at 16 epochs.\n",
      "Epoch:[17/100], Step: [0] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[17/100], Step: [50] learning_rate: 0.0001, loss: 0.0362\n",
      "Epoch:[17/100], Step: [100] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[17/100], Step: [150] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[17/100], Step: [200] learning_rate: 0.0001, loss: 0.0334\n",
      "Epoch:[17/100], Step: [250] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[17/100], Step: [300] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[17/100], Step: [350] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[17/100], Step: [400] learning_rate: 0.0001, loss: 0.0350\n",
      "Epoch:[17/100], Step: [450] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[17/100], Step: [500] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[17/100], Step: [550] learning_rate: 0.0001, loss: 0.0342\n",
      "Epoch:[17/100], Step: [600] learning_rate: 0.0001, loss: 0.0348\n",
      "Average MSE loss on test dataset: 0.0321\n",
      "Saving checkpoints at 17 epochs.\n",
      "Epoch:[18/100], Step: [0] learning_rate: 0.0001, loss: 0.0333\n",
      "Epoch:[18/100], Step: [50] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[18/100], Step: [100] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[18/100], Step: [150] learning_rate: 0.0001, loss: 0.0338\n",
      "Epoch:[18/100], Step: [200] learning_rate: 0.0001, loss: 0.0328\n",
      "Epoch:[18/100], Step: [250] learning_rate: 0.0001, loss: 0.0336\n",
      "Epoch:[18/100], Step: [300] learning_rate: 0.0001, loss: 0.0343\n",
      "Epoch:[18/100], Step: [350] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[18/100], Step: [400] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[18/100], Step: [450] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[18/100], Step: [500] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[18/100], Step: [550] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[18/100], Step: [600] learning_rate: 0.0001, loss: 0.0356\n",
      "Average MSE loss on test dataset: 0.0323\n",
      "Saving checkpoints at 18 epochs.\n",
      "Epoch:[19/100], Step: [0] learning_rate: 0.0001, loss: 0.0338\n",
      "Epoch:[19/100], Step: [50] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[19/100], Step: [100] learning_rate: 0.0001, loss: 0.0357\n",
      "Epoch:[19/100], Step: [150] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[19/100], Step: [200] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[19/100], Step: [250] learning_rate: 0.0001, loss: 0.0336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[19/100], Step: [300] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[19/100], Step: [350] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[19/100], Step: [400] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[19/100], Step: [450] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[19/100], Step: [500] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[19/100], Step: [550] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[19/100], Step: [600] learning_rate: 0.0001, loss: 0.0322\n",
      "Average MSE loss on test dataset: 0.0319\n",
      "Saving checkpoints at 19 epochs.\n",
      "Epoch:[20/100], Step: [0] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[20/100], Step: [50] learning_rate: 0.0001, loss: 0.0350\n",
      "Epoch:[20/100], Step: [100] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[20/100], Step: [150] learning_rate: 0.0001, loss: 0.0324\n",
      "Epoch:[20/100], Step: [200] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[20/100], Step: [250] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[20/100], Step: [300] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[20/100], Step: [350] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[20/100], Step: [400] learning_rate: 0.0001, loss: 0.0353\n",
      "Epoch:[20/100], Step: [450] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[20/100], Step: [500] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[20/100], Step: [550] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[20/100], Step: [600] learning_rate: 0.0001, loss: 0.0297\n",
      "Average MSE loss on test dataset: 0.0318\n",
      "Saving checkpoints at 20 epochs.\n",
      "Epoch:[21/100], Step: [0] learning_rate: 0.0001, loss: 0.0326\n",
      "Epoch:[21/100], Step: [50] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[21/100], Step: [100] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[21/100], Step: [150] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[21/100], Step: [200] learning_rate: 0.0001, loss: 0.0332\n",
      "Epoch:[21/100], Step: [250] learning_rate: 0.0001, loss: 0.0264\n",
      "Epoch:[21/100], Step: [300] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[21/100], Step: [350] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[21/100], Step: [400] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[21/100], Step: [450] learning_rate: 0.0001, loss: 0.0277\n",
      "Epoch:[21/100], Step: [500] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[21/100], Step: [550] learning_rate: 0.0001, loss: 0.0346\n",
      "Epoch:[21/100], Step: [600] learning_rate: 0.0001, loss: 0.0313\n",
      "Average MSE loss on test dataset: 0.0318\n",
      "Saving checkpoints at 21 epochs.\n",
      "Epoch:[22/100], Step: [0] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[22/100], Step: [50] learning_rate: 0.0001, loss: 0.0276\n",
      "Epoch:[22/100], Step: [100] learning_rate: 0.0001, loss: 0.0344\n",
      "Epoch:[22/100], Step: [150] learning_rate: 0.0001, loss: 0.0341\n",
      "Epoch:[22/100], Step: [200] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[22/100], Step: [250] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[22/100], Step: [300] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[22/100], Step: [350] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[22/100], Step: [400] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[22/100], Step: [450] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[22/100], Step: [500] learning_rate: 0.0001, loss: 0.0329\n",
      "Epoch:[22/100], Step: [550] learning_rate: 0.0001, loss: 0.0333\n",
      "Epoch:[22/100], Step: [600] learning_rate: 0.0001, loss: 0.0313\n",
      "Average MSE loss on test dataset: 0.0316\n",
      "Saving checkpoints at 22 epochs.\n",
      "Epoch:[23/100], Step: [0] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[23/100], Step: [50] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[23/100], Step: [100] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[23/100], Step: [150] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[23/100], Step: [200] learning_rate: 0.0001, loss: 0.0319\n",
      "Epoch:[23/100], Step: [250] learning_rate: 0.0001, loss: 0.0324\n",
      "Epoch:[23/100], Step: [300] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[23/100], Step: [350] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[23/100], Step: [400] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[23/100], Step: [450] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[23/100], Step: [500] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[23/100], Step: [550] learning_rate: 0.0001, loss: 0.0326\n",
      "Epoch:[23/100], Step: [600] learning_rate: 0.0001, loss: 0.0311\n",
      "Average MSE loss on test dataset: 0.0316\n",
      "Saving checkpoints at 23 epochs.\n",
      "Epoch:[24/100], Step: [0] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[24/100], Step: [50] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[24/100], Step: [100] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[24/100], Step: [150] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[24/100], Step: [200] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[24/100], Step: [250] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[24/100], Step: [300] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[24/100], Step: [350] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[24/100], Step: [400] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[24/100], Step: [450] learning_rate: 0.0001, loss: 0.0351\n",
      "Epoch:[24/100], Step: [500] learning_rate: 0.0001, loss: 0.0328\n",
      "Epoch:[24/100], Step: [550] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[24/100], Step: [600] learning_rate: 0.0001, loss: 0.0264\n",
      "Average MSE loss on test dataset: 0.0315\n",
      "Saving checkpoints at 24 epochs.\n",
      "Epoch:[25/100], Step: [0] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[25/100], Step: [50] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[25/100], Step: [100] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[25/100], Step: [150] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[25/100], Step: [200] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[25/100], Step: [250] learning_rate: 0.0001, loss: 0.0328\n",
      "Epoch:[25/100], Step: [300] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[25/100], Step: [350] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[25/100], Step: [400] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[25/100], Step: [450] learning_rate: 0.0001, loss: 0.0336\n",
      "Epoch:[25/100], Step: [500] learning_rate: 0.0001, loss: 0.0326\n",
      "Epoch:[25/100], Step: [550] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[25/100], Step: [600] learning_rate: 0.0001, loss: 0.0307\n",
      "Average MSE loss on test dataset: 0.0315\n",
      "Saving checkpoints at 25 epochs.\n",
      "Epoch:[26/100], Step: [0] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[26/100], Step: [50] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[26/100], Step: [100] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[26/100], Step: [150] learning_rate: 0.0001, loss: 0.0337\n",
      "Epoch:[26/100], Step: [200] learning_rate: 0.0001, loss: 0.0338\n",
      "Epoch:[26/100], Step: [250] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[26/100], Step: [300] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[26/100], Step: [350] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[26/100], Step: [400] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[26/100], Step: [450] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[26/100], Step: [500] learning_rate: 0.0001, loss: 0.0326\n",
      "Epoch:[26/100], Step: [550] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[26/100], Step: [600] learning_rate: 0.0001, loss: 0.0318\n",
      "Average MSE loss on test dataset: 0.0313\n",
      "Saving checkpoints at 26 epochs.\n",
      "Epoch:[27/100], Step: [0] learning_rate: 0.0001, loss: 0.0273\n",
      "Epoch:[27/100], Step: [50] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[27/100], Step: [100] learning_rate: 0.0001, loss: 0.0314\n",
      "Epoch:[27/100], Step: [150] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[27/100], Step: [200] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[27/100], Step: [250] learning_rate: 0.0001, loss: 0.0329\n",
      "Epoch:[27/100], Step: [300] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[27/100], Step: [350] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[27/100], Step: [400] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[27/100], Step: [450] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[27/100], Step: [500] learning_rate: 0.0001, loss: 0.0308\n",
      "Epoch:[27/100], Step: [550] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[27/100], Step: [600] learning_rate: 0.0001, loss: 0.0315\n",
      "Average MSE loss on test dataset: 0.0313\n",
      "Saving checkpoints at 27 epochs.\n",
      "Epoch:[28/100], Step: [0] learning_rate: 0.0001, loss: 0.0308\n",
      "Epoch:[28/100], Step: [50] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[28/100], Step: [100] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[28/100], Step: [150] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[28/100], Step: [200] learning_rate: 0.0001, loss: 0.0338\n",
      "Epoch:[28/100], Step: [250] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[28/100], Step: [300] learning_rate: 0.0001, loss: 0.0327\n",
      "Epoch:[28/100], Step: [350] learning_rate: 0.0001, loss: 0.0313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[28/100], Step: [400] learning_rate: 0.0001, loss: 0.0326\n",
      "Epoch:[28/100], Step: [450] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[28/100], Step: [500] learning_rate: 0.0001, loss: 0.0326\n",
      "Epoch:[28/100], Step: [550] learning_rate: 0.0001, loss: 0.0327\n",
      "Epoch:[28/100], Step: [600] learning_rate: 0.0001, loss: 0.0321\n",
      "Average MSE loss on test dataset: 0.0313\n",
      "Saving checkpoints at 28 epochs.\n",
      "Epoch:[29/100], Step: [0] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[29/100], Step: [50] learning_rate: 0.0001, loss: 0.0350\n",
      "Epoch:[29/100], Step: [100] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[29/100], Step: [150] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[29/100], Step: [200] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[29/100], Step: [250] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[29/100], Step: [300] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[29/100], Step: [350] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[29/100], Step: [400] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[29/100], Step: [450] learning_rate: 0.0001, loss: 0.0314\n",
      "Epoch:[29/100], Step: [500] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[29/100], Step: [550] learning_rate: 0.0001, loss: 0.0267\n",
      "Epoch:[29/100], Step: [600] learning_rate: 0.0001, loss: 0.0312\n",
      "Average MSE loss on test dataset: 0.0312\n",
      "Saving checkpoints at 29 epochs.\n",
      "Epoch:[30/100], Step: [0] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[30/100], Step: [50] learning_rate: 0.0001, loss: 0.0330\n",
      "Epoch:[30/100], Step: [100] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[30/100], Step: [150] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[30/100], Step: [200] learning_rate: 0.0001, loss: 0.0341\n",
      "Epoch:[30/100], Step: [250] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[30/100], Step: [300] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[30/100], Step: [350] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[30/100], Step: [400] learning_rate: 0.0001, loss: 0.0337\n",
      "Epoch:[30/100], Step: [450] learning_rate: 0.0001, loss: 0.0336\n",
      "Epoch:[30/100], Step: [500] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[30/100], Step: [550] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[30/100], Step: [600] learning_rate: 0.0001, loss: 0.0310\n",
      "Average MSE loss on test dataset: 0.0311\n",
      "Saving checkpoints at 30 epochs.\n",
      "Epoch:[31/100], Step: [0] learning_rate: 0.0001, loss: 0.0327\n",
      "Epoch:[31/100], Step: [50] learning_rate: 0.0001, loss: 0.0334\n",
      "Epoch:[31/100], Step: [100] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[31/100], Step: [150] learning_rate: 0.0001, loss: 0.0326\n",
      "Epoch:[31/100], Step: [200] learning_rate: 0.0001, loss: 0.0314\n",
      "Epoch:[31/100], Step: [250] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[31/100], Step: [300] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[31/100], Step: [350] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[31/100], Step: [400] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[31/100], Step: [450] learning_rate: 0.0001, loss: 0.0331\n",
      "Epoch:[31/100], Step: [500] learning_rate: 0.0001, loss: 0.0324\n",
      "Epoch:[31/100], Step: [550] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[31/100], Step: [600] learning_rate: 0.0001, loss: 0.0309\n",
      "Average MSE loss on test dataset: 0.0310\n",
      "Saving checkpoints at 31 epochs.\n",
      "Epoch:[32/100], Step: [0] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[32/100], Step: [50] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[32/100], Step: [100] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[32/100], Step: [150] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[32/100], Step: [200] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[32/100], Step: [250] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[32/100], Step: [300] learning_rate: 0.0001, loss: 0.0360\n",
      "Epoch:[32/100], Step: [350] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[32/100], Step: [400] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[32/100], Step: [450] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[32/100], Step: [500] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[32/100], Step: [550] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[32/100], Step: [600] learning_rate: 0.0001, loss: 0.0324\n",
      "Average MSE loss on test dataset: 0.0310\n",
      "Saving checkpoints at 32 epochs.\n",
      "Epoch:[33/100], Step: [0] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[33/100], Step: [50] learning_rate: 0.0001, loss: 0.0340\n",
      "Epoch:[33/100], Step: [100] learning_rate: 0.0001, loss: 0.0355\n",
      "Epoch:[33/100], Step: [150] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[33/100], Step: [200] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[33/100], Step: [250] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[33/100], Step: [300] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[33/100], Step: [350] learning_rate: 0.0001, loss: 0.0324\n",
      "Epoch:[33/100], Step: [400] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[33/100], Step: [450] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[33/100], Step: [500] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[33/100], Step: [550] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[33/100], Step: [600] learning_rate: 0.0001, loss: 0.0308\n",
      "Average MSE loss on test dataset: 0.0310\n",
      "Saving checkpoints at 33 epochs.\n",
      "Epoch:[34/100], Step: [0] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[34/100], Step: [50] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[34/100], Step: [100] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[34/100], Step: [150] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[34/100], Step: [200] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[34/100], Step: [250] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[34/100], Step: [300] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[34/100], Step: [350] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[34/100], Step: [400] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[34/100], Step: [450] learning_rate: 0.0001, loss: 0.0342\n",
      "Epoch:[34/100], Step: [500] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[34/100], Step: [550] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[34/100], Step: [600] learning_rate: 0.0001, loss: 0.0308\n",
      "Average MSE loss on test dataset: 0.0309\n",
      "Saving checkpoints at 34 epochs.\n",
      "Epoch:[35/100], Step: [0] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[35/100], Step: [50] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[35/100], Step: [100] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[35/100], Step: [150] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[35/100], Step: [200] learning_rate: 0.0001, loss: 0.0314\n",
      "Epoch:[35/100], Step: [250] learning_rate: 0.0001, loss: 0.0272\n",
      "Epoch:[35/100], Step: [300] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[35/100], Step: [350] learning_rate: 0.0001, loss: 0.0355\n",
      "Epoch:[35/100], Step: [400] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[35/100], Step: [450] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[35/100], Step: [500] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[35/100], Step: [550] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[35/100], Step: [600] learning_rate: 0.0001, loss: 0.0300\n",
      "Average MSE loss on test dataset: 0.0308\n",
      "Saving checkpoints at 35 epochs.\n",
      "Epoch:[36/100], Step: [0] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[36/100], Step: [50] learning_rate: 0.0001, loss: 0.0324\n",
      "Epoch:[36/100], Step: [100] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[36/100], Step: [150] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[36/100], Step: [200] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[36/100], Step: [250] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[36/100], Step: [300] learning_rate: 0.0001, loss: 0.0276\n",
      "Epoch:[36/100], Step: [350] learning_rate: 0.0001, loss: 0.0328\n",
      "Epoch:[36/100], Step: [400] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[36/100], Step: [450] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[36/100], Step: [500] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[36/100], Step: [550] learning_rate: 0.0001, loss: 0.0324\n",
      "Epoch:[36/100], Step: [600] learning_rate: 0.0001, loss: 0.0266\n",
      "Average MSE loss on test dataset: 0.0307\n",
      "Saving checkpoints at 36 epochs.\n",
      "Epoch:[37/100], Step: [0] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[37/100], Step: [50] learning_rate: 0.0001, loss: 0.0339\n",
      "Epoch:[37/100], Step: [100] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[37/100], Step: [150] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[37/100], Step: [200] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[37/100], Step: [250] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[37/100], Step: [300] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[37/100], Step: [350] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[37/100], Step: [400] learning_rate: 0.0001, loss: 0.0329\n",
      "Epoch:[37/100], Step: [450] learning_rate: 0.0001, loss: 0.0319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[37/100], Step: [500] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[37/100], Step: [550] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[37/100], Step: [600] learning_rate: 0.0001, loss: 0.0294\n",
      "Average MSE loss on test dataset: 0.0308\n",
      "Saving checkpoints at 37 epochs.\n",
      "Epoch:[38/100], Step: [0] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[38/100], Step: [50] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[38/100], Step: [100] learning_rate: 0.0001, loss: 0.0330\n",
      "Epoch:[38/100], Step: [150] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[38/100], Step: [200] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[38/100], Step: [250] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[38/100], Step: [300] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[38/100], Step: [350] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[38/100], Step: [400] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[38/100], Step: [450] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[38/100], Step: [500] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[38/100], Step: [550] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[38/100], Step: [600] learning_rate: 0.0001, loss: 0.0303\n",
      "Average MSE loss on test dataset: 0.0306\n",
      "Saving checkpoints at 38 epochs.\n",
      "Epoch:[39/100], Step: [0] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[39/100], Step: [50] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[39/100], Step: [100] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[39/100], Step: [150] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[39/100], Step: [200] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[39/100], Step: [250] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[39/100], Step: [300] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[39/100], Step: [350] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[39/100], Step: [400] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[39/100], Step: [450] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[39/100], Step: [500] learning_rate: 0.0001, loss: 0.0335\n",
      "Epoch:[39/100], Step: [550] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[39/100], Step: [600] learning_rate: 0.0001, loss: 0.0306\n",
      "Average MSE loss on test dataset: 0.0305\n",
      "Saving checkpoints at 39 epochs.\n",
      "Epoch:[40/100], Step: [0] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[40/100], Step: [50] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[40/100], Step: [100] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[40/100], Step: [150] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[40/100], Step: [200] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[40/100], Step: [250] learning_rate: 0.0001, loss: 0.0308\n",
      "Epoch:[40/100], Step: [300] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[40/100], Step: [350] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[40/100], Step: [400] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[40/100], Step: [450] learning_rate: 0.0001, loss: 0.0332\n",
      "Epoch:[40/100], Step: [500] learning_rate: 0.0001, loss: 0.0319\n",
      "Epoch:[40/100], Step: [550] learning_rate: 0.0001, loss: 0.0328\n",
      "Epoch:[40/100], Step: [600] learning_rate: 0.0001, loss: 0.0322\n",
      "Average MSE loss on test dataset: 0.0305\n",
      "Saving checkpoints at 40 epochs.\n",
      "Epoch:[41/100], Step: [0] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[41/100], Step: [50] learning_rate: 0.0001, loss: 0.0308\n",
      "Epoch:[41/100], Step: [100] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[41/100], Step: [150] learning_rate: 0.0001, loss: 0.0334\n",
      "Epoch:[41/100], Step: [200] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[41/100], Step: [250] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[41/100], Step: [300] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[41/100], Step: [350] learning_rate: 0.0001, loss: 0.0280\n",
      "Epoch:[41/100], Step: [400] learning_rate: 0.0001, loss: 0.0338\n",
      "Epoch:[41/100], Step: [450] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[41/100], Step: [500] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[41/100], Step: [550] learning_rate: 0.0001, loss: 0.0267\n",
      "Epoch:[41/100], Step: [600] learning_rate: 0.0001, loss: 0.0299\n",
      "Average MSE loss on test dataset: 0.0305\n",
      "Saving checkpoints at 41 epochs.\n",
      "Epoch:[42/100], Step: [0] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[42/100], Step: [50] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[42/100], Step: [100] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[42/100], Step: [150] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[42/100], Step: [200] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[42/100], Step: [250] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[42/100], Step: [300] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[42/100], Step: [350] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[42/100], Step: [400] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[42/100], Step: [450] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[42/100], Step: [500] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[42/100], Step: [550] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[42/100], Step: [600] learning_rate: 0.0001, loss: 0.0331\n",
      "Average MSE loss on test dataset: 0.0304\n",
      "Saving checkpoints at 42 epochs.\n",
      "Epoch:[43/100], Step: [0] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[43/100], Step: [50] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[43/100], Step: [100] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[43/100], Step: [150] learning_rate: 0.0001, loss: 0.0308\n",
      "Epoch:[43/100], Step: [200] learning_rate: 0.0001, loss: 0.0268\n",
      "Epoch:[43/100], Step: [250] learning_rate: 0.0001, loss: 0.0276\n",
      "Epoch:[43/100], Step: [300] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[43/100], Step: [350] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[43/100], Step: [400] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[43/100], Step: [450] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[43/100], Step: [500] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[43/100], Step: [550] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[43/100], Step: [600] learning_rate: 0.0001, loss: 0.0284\n",
      "Average MSE loss on test dataset: 0.0303\n",
      "Saving checkpoints at 43 epochs.\n",
      "Epoch:[44/100], Step: [0] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[44/100], Step: [50] learning_rate: 0.0001, loss: 0.0280\n",
      "Epoch:[44/100], Step: [100] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[44/100], Step: [150] learning_rate: 0.0001, loss: 0.0266\n",
      "Epoch:[44/100], Step: [200] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[44/100], Step: [250] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[44/100], Step: [300] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[44/100], Step: [350] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[44/100], Step: [400] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[44/100], Step: [450] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[44/100], Step: [500] learning_rate: 0.0001, loss: 0.0276\n",
      "Epoch:[44/100], Step: [550] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[44/100], Step: [600] learning_rate: 0.0001, loss: 0.0287\n",
      "Average MSE loss on test dataset: 0.0303\n",
      "Saving checkpoints at 44 epochs.\n",
      "Epoch:[45/100], Step: [0] learning_rate: 0.0001, loss: 0.0343\n",
      "Epoch:[45/100], Step: [50] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[45/100], Step: [100] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[45/100], Step: [150] learning_rate: 0.0001, loss: 0.0319\n",
      "Epoch:[45/100], Step: [200] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[45/100], Step: [250] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[45/100], Step: [300] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[45/100], Step: [350] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[45/100], Step: [400] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[45/100], Step: [450] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[45/100], Step: [500] learning_rate: 0.0001, loss: 0.0357\n",
      "Epoch:[45/100], Step: [550] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[45/100], Step: [600] learning_rate: 0.0001, loss: 0.0298\n",
      "Average MSE loss on test dataset: 0.0303\n",
      "Saving checkpoints at 45 epochs.\n",
      "Epoch:[46/100], Step: [0] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[46/100], Step: [50] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[46/100], Step: [100] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[46/100], Step: [150] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[46/100], Step: [200] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[46/100], Step: [250] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[46/100], Step: [300] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[46/100], Step: [350] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[46/100], Step: [400] learning_rate: 0.0001, loss: 0.0334\n",
      "Epoch:[46/100], Step: [450] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[46/100], Step: [500] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[46/100], Step: [550] learning_rate: 0.0001, loss: 0.0298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[46/100], Step: [600] learning_rate: 0.0001, loss: 0.0319\n",
      "Average MSE loss on test dataset: 0.0303\n",
      "Saving checkpoints at 46 epochs.\n",
      "Epoch:[47/100], Step: [0] learning_rate: 0.0001, loss: 0.0272\n",
      "Epoch:[47/100], Step: [50] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[47/100], Step: [100] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[47/100], Step: [150] learning_rate: 0.0001, loss: 0.0308\n",
      "Epoch:[47/100], Step: [200] learning_rate: 0.0001, loss: 0.0273\n",
      "Epoch:[47/100], Step: [250] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[47/100], Step: [300] learning_rate: 0.0001, loss: 0.0314\n",
      "Epoch:[47/100], Step: [350] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[47/100], Step: [400] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[47/100], Step: [450] learning_rate: 0.0001, loss: 0.0314\n",
      "Epoch:[47/100], Step: [500] learning_rate: 0.0001, loss: 0.0271\n",
      "Epoch:[47/100], Step: [550] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[47/100], Step: [600] learning_rate: 0.0001, loss: 0.0302\n",
      "Average MSE loss on test dataset: 0.0301\n",
      "Saving checkpoints at 47 epochs.\n",
      "Epoch:[48/100], Step: [0] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[48/100], Step: [50] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[48/100], Step: [100] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[48/100], Step: [150] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[48/100], Step: [200] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[48/100], Step: [250] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[48/100], Step: [300] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[48/100], Step: [350] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[48/100], Step: [400] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[48/100], Step: [450] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[48/100], Step: [500] learning_rate: 0.0001, loss: 0.0332\n",
      "Epoch:[48/100], Step: [550] learning_rate: 0.0001, loss: 0.0319\n",
      "Epoch:[48/100], Step: [600] learning_rate: 0.0001, loss: 0.0280\n",
      "Average MSE loss on test dataset: 0.0300\n",
      "Saving checkpoints at 48 epochs.\n",
      "Epoch:[49/100], Step: [0] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[49/100], Step: [50] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[49/100], Step: [100] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[49/100], Step: [150] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[49/100], Step: [200] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[49/100], Step: [250] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[49/100], Step: [300] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[49/100], Step: [350] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[49/100], Step: [400] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[49/100], Step: [450] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[49/100], Step: [500] learning_rate: 0.0001, loss: 0.0261\n",
      "Epoch:[49/100], Step: [550] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[49/100], Step: [600] learning_rate: 0.0001, loss: 0.0314\n",
      "Average MSE loss on test dataset: 0.0300\n",
      "Saving checkpoints at 49 epochs.\n",
      "Epoch:[50/100], Step: [0] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[50/100], Step: [50] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[50/100], Step: [100] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[50/100], Step: [150] learning_rate: 0.0001, loss: 0.0280\n",
      "Epoch:[50/100], Step: [200] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[50/100], Step: [250] learning_rate: 0.0001, loss: 0.0261\n",
      "Epoch:[50/100], Step: [300] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[50/100], Step: [350] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[50/100], Step: [400] learning_rate: 0.0001, loss: 0.0284\n",
      "Epoch:[50/100], Step: [450] learning_rate: 0.0001, loss: 0.0277\n",
      "Epoch:[50/100], Step: [500] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[50/100], Step: [550] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[50/100], Step: [600] learning_rate: 0.0001, loss: 0.0281\n",
      "Average MSE loss on test dataset: 0.0299\n",
      "Saving checkpoints at 50 epochs.\n",
      "Epoch:[51/100], Step: [0] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[51/100], Step: [50] learning_rate: 0.0001, loss: 0.0273\n",
      "Epoch:[51/100], Step: [100] learning_rate: 0.0001, loss: 0.0264\n",
      "Epoch:[51/100], Step: [150] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[51/100], Step: [200] learning_rate: 0.0001, loss: 0.0326\n",
      "Epoch:[51/100], Step: [250] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[51/100], Step: [300] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[51/100], Step: [350] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[51/100], Step: [400] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[51/100], Step: [450] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[51/100], Step: [500] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[51/100], Step: [550] learning_rate: 0.0001, loss: 0.0253\n",
      "Epoch:[51/100], Step: [600] learning_rate: 0.0001, loss: 0.0319\n",
      "Average MSE loss on test dataset: 0.0299\n",
      "Saving checkpoints at 51 epochs.\n",
      "Epoch:[52/100], Step: [0] learning_rate: 0.0001, loss: 0.0327\n",
      "Epoch:[52/100], Step: [50] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[52/100], Step: [100] learning_rate: 0.0001, loss: 0.0319\n",
      "Epoch:[52/100], Step: [150] learning_rate: 0.0001, loss: 0.0308\n",
      "Epoch:[52/100], Step: [200] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[52/100], Step: [250] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[52/100], Step: [300] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[52/100], Step: [350] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[52/100], Step: [400] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[52/100], Step: [450] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[52/100], Step: [500] learning_rate: 0.0001, loss: 0.0308\n",
      "Epoch:[52/100], Step: [550] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[52/100], Step: [600] learning_rate: 0.0001, loss: 0.0296\n",
      "Average MSE loss on test dataset: 0.0298\n",
      "Saving checkpoints at 52 epochs.\n",
      "Epoch:[53/100], Step: [0] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[53/100], Step: [50] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[53/100], Step: [100] learning_rate: 0.0001, loss: 0.0252\n",
      "Epoch:[53/100], Step: [150] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[53/100], Step: [200] learning_rate: 0.0001, loss: 0.0262\n",
      "Epoch:[53/100], Step: [250] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[53/100], Step: [300] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[53/100], Step: [350] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[53/100], Step: [400] learning_rate: 0.0001, loss: 0.0268\n",
      "Epoch:[53/100], Step: [450] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[53/100], Step: [500] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[53/100], Step: [550] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[53/100], Step: [600] learning_rate: 0.0001, loss: 0.0302\n",
      "Average MSE loss on test dataset: 0.0297\n",
      "Saving checkpoints at 53 epochs.\n",
      "Epoch:[54/100], Step: [0] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[54/100], Step: [50] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[54/100], Step: [100] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[54/100], Step: [150] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[54/100], Step: [200] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[54/100], Step: [250] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[54/100], Step: [300] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[54/100], Step: [350] learning_rate: 0.0001, loss: 0.0280\n",
      "Epoch:[54/100], Step: [400] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[54/100], Step: [450] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[54/100], Step: [500] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[54/100], Step: [550] learning_rate: 0.0001, loss: 0.0271\n",
      "Epoch:[54/100], Step: [600] learning_rate: 0.0001, loss: 0.0306\n",
      "Average MSE loss on test dataset: 0.0297\n",
      "Saving checkpoints at 54 epochs.\n",
      "Epoch:[55/100], Step: [0] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[55/100], Step: [50] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[55/100], Step: [100] learning_rate: 0.0001, loss: 0.0280\n",
      "Epoch:[55/100], Step: [150] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[55/100], Step: [200] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[55/100], Step: [250] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[55/100], Step: [300] learning_rate: 0.0001, loss: 0.0269\n",
      "Epoch:[55/100], Step: [350] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[55/100], Step: [400] learning_rate: 0.0001, loss: 0.0330\n",
      "Epoch:[55/100], Step: [450] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[55/100], Step: [500] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[55/100], Step: [550] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[55/100], Step: [600] learning_rate: 0.0001, loss: 0.0288\n",
      "Average MSE loss on test dataset: 0.0296\n",
      "Saving checkpoints at 55 epochs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[56/100], Step: [0] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[56/100], Step: [50] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[56/100], Step: [100] learning_rate: 0.0001, loss: 0.0335\n",
      "Epoch:[56/100], Step: [150] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[56/100], Step: [200] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[56/100], Step: [250] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[56/100], Step: [300] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[56/100], Step: [350] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[56/100], Step: [400] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[56/100], Step: [450] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[56/100], Step: [500] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[56/100], Step: [550] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[56/100], Step: [600] learning_rate: 0.0001, loss: 0.0304\n",
      "Average MSE loss on test dataset: 0.0295\n",
      "Saving checkpoints at 56 epochs.\n",
      "Epoch:[57/100], Step: [0] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[57/100], Step: [50] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[57/100], Step: [100] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[57/100], Step: [150] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[57/100], Step: [200] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[57/100], Step: [250] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[57/100], Step: [300] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[57/100], Step: [350] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[57/100], Step: [400] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[57/100], Step: [450] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[57/100], Step: [500] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[57/100], Step: [550] learning_rate: 0.0001, loss: 0.0308\n",
      "Epoch:[57/100], Step: [600] learning_rate: 0.0001, loss: 0.0282\n",
      "Average MSE loss on test dataset: 0.0294\n",
      "Saving checkpoints at 57 epochs.\n",
      "Epoch:[58/100], Step: [0] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[58/100], Step: [50] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[58/100], Step: [100] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[58/100], Step: [150] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[58/100], Step: [200] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[58/100], Step: [250] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[58/100], Step: [300] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[58/100], Step: [350] learning_rate: 0.0001, loss: 0.0273\n",
      "Epoch:[58/100], Step: [400] learning_rate: 0.0001, loss: 0.0277\n",
      "Epoch:[58/100], Step: [450] learning_rate: 0.0001, loss: 0.0266\n",
      "Epoch:[58/100], Step: [500] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[58/100], Step: [550] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[58/100], Step: [600] learning_rate: 0.0001, loss: 0.0276\n",
      "Average MSE loss on test dataset: 0.0294\n",
      "Saving checkpoints at 58 epochs.\n",
      "Epoch:[59/100], Step: [0] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[59/100], Step: [50] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[59/100], Step: [100] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[59/100], Step: [150] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[59/100], Step: [200] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[59/100], Step: [250] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[59/100], Step: [300] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[59/100], Step: [350] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[59/100], Step: [400] learning_rate: 0.0001, loss: 0.0276\n",
      "Epoch:[59/100], Step: [450] learning_rate: 0.0001, loss: 0.0284\n",
      "Epoch:[59/100], Step: [500] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[59/100], Step: [550] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[59/100], Step: [600] learning_rate: 0.0001, loss: 0.0270\n",
      "Average MSE loss on test dataset: 0.0295\n",
      "Saving checkpoints at 59 epochs.\n",
      "Epoch:[60/100], Step: [0] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[60/100], Step: [50] learning_rate: 0.0001, loss: 0.0327\n",
      "Epoch:[60/100], Step: [100] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[60/100], Step: [150] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[60/100], Step: [200] learning_rate: 0.0001, loss: 0.0251\n",
      "Epoch:[60/100], Step: [250] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[60/100], Step: [300] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[60/100], Step: [350] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[60/100], Step: [400] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[60/100], Step: [450] learning_rate: 0.0001, loss: 0.0277\n",
      "Epoch:[60/100], Step: [500] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[60/100], Step: [550] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[60/100], Step: [600] learning_rate: 0.0001, loss: 0.0292\n",
      "Average MSE loss on test dataset: 0.0292\n",
      "Saving checkpoints at 60 epochs.\n",
      "Epoch:[61/100], Step: [0] learning_rate: 0.0001, loss: 0.0274\n",
      "Epoch:[61/100], Step: [50] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[61/100], Step: [100] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[61/100], Step: [150] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[61/100], Step: [200] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[61/100], Step: [250] learning_rate: 0.0001, loss: 0.0265\n",
      "Epoch:[61/100], Step: [300] learning_rate: 0.0001, loss: 0.0268\n",
      "Epoch:[61/100], Step: [350] learning_rate: 0.0001, loss: 0.0284\n",
      "Epoch:[61/100], Step: [400] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[61/100], Step: [450] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[61/100], Step: [500] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[61/100], Step: [550] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[61/100], Step: [600] learning_rate: 0.0001, loss: 0.0288\n",
      "Average MSE loss on test dataset: 0.0291\n",
      "Saving checkpoints at 61 epochs.\n",
      "Epoch:[62/100], Step: [0] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[62/100], Step: [50] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[62/100], Step: [100] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[62/100], Step: [150] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[62/100], Step: [200] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[62/100], Step: [250] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[62/100], Step: [300] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[62/100], Step: [350] learning_rate: 0.0001, loss: 0.0267\n",
      "Epoch:[62/100], Step: [400] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[62/100], Step: [450] learning_rate: 0.0001, loss: 0.0273\n",
      "Epoch:[62/100], Step: [500] learning_rate: 0.0001, loss: 0.0284\n",
      "Epoch:[62/100], Step: [550] learning_rate: 0.0001, loss: 0.0255\n",
      "Epoch:[62/100], Step: [600] learning_rate: 0.0001, loss: 0.0331\n",
      "Average MSE loss on test dataset: 0.0291\n",
      "Saving checkpoints at 62 epochs.\n",
      "Epoch:[63/100], Step: [0] learning_rate: 0.0001, loss: 0.0271\n",
      "Epoch:[63/100], Step: [50] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[63/100], Step: [100] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[63/100], Step: [150] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[63/100], Step: [200] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[63/100], Step: [250] learning_rate: 0.0001, loss: 0.0262\n",
      "Epoch:[63/100], Step: [300] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[63/100], Step: [350] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[63/100], Step: [400] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[63/100], Step: [450] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[63/100], Step: [500] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[63/100], Step: [550] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[63/100], Step: [600] learning_rate: 0.0001, loss: 0.0287\n",
      "Average MSE loss on test dataset: 0.0292\n",
      "Saving checkpoints at 63 epochs.\n",
      "Epoch:[64/100], Step: [0] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[64/100], Step: [50] learning_rate: 0.0001, loss: 0.0277\n",
      "Epoch:[64/100], Step: [100] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[64/100], Step: [150] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[64/100], Step: [200] learning_rate: 0.0001, loss: 0.0276\n",
      "Epoch:[64/100], Step: [250] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[64/100], Step: [300] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[64/100], Step: [350] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[64/100], Step: [400] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[64/100], Step: [450] learning_rate: 0.0001, loss: 0.0277\n",
      "Epoch:[64/100], Step: [500] learning_rate: 0.0001, loss: 0.0272\n",
      "Epoch:[64/100], Step: [550] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[64/100], Step: [600] learning_rate: 0.0001, loss: 0.0297\n",
      "Average MSE loss on test dataset: 0.0291\n",
      "Saving checkpoints at 64 epochs.\n",
      "Epoch:[65/100], Step: [0] learning_rate: 0.0001, loss: 0.0269\n",
      "Epoch:[65/100], Step: [50] learning_rate: 0.0001, loss: 0.0289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[65/100], Step: [100] learning_rate: 0.0001, loss: 0.0308\n",
      "Epoch:[65/100], Step: [150] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[65/100], Step: [200] learning_rate: 0.0001, loss: 0.0319\n",
      "Epoch:[65/100], Step: [250] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[65/100], Step: [300] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[65/100], Step: [350] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[65/100], Step: [400] learning_rate: 0.0001, loss: 0.0271\n",
      "Epoch:[65/100], Step: [450] learning_rate: 0.0001, loss: 0.0270\n",
      "Epoch:[65/100], Step: [500] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[65/100], Step: [550] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[65/100], Step: [600] learning_rate: 0.0001, loss: 0.0274\n",
      "Average MSE loss on test dataset: 0.0289\n",
      "Saving checkpoints at 65 epochs.\n",
      "Epoch:[66/100], Step: [0] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[66/100], Step: [50] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[66/100], Step: [100] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[66/100], Step: [150] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[66/100], Step: [200] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[66/100], Step: [250] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[66/100], Step: [300] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[66/100], Step: [350] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[66/100], Step: [400] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[66/100], Step: [450] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[66/100], Step: [500] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[66/100], Step: [550] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[66/100], Step: [600] learning_rate: 0.0001, loss: 0.0264\n",
      "Average MSE loss on test dataset: 0.0293\n",
      "Saving checkpoints at 66 epochs.\n",
      "Epoch:[67/100], Step: [0] learning_rate: 0.0001, loss: 0.0269\n",
      "Epoch:[67/100], Step: [50] learning_rate: 0.0001, loss: 0.0274\n",
      "Epoch:[67/100], Step: [100] learning_rate: 0.0001, loss: 0.0264\n",
      "Epoch:[67/100], Step: [150] learning_rate: 0.0001, loss: 0.0277\n",
      "Epoch:[67/100], Step: [200] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[67/100], Step: [250] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[67/100], Step: [300] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[67/100], Step: [350] learning_rate: 0.0001, loss: 0.0270\n",
      "Epoch:[67/100], Step: [400] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[67/100], Step: [450] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[67/100], Step: [500] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[67/100], Step: [550] learning_rate: 0.0001, loss: 0.0264\n",
      "Epoch:[67/100], Step: [600] learning_rate: 0.0001, loss: 0.0284\n",
      "Average MSE loss on test dataset: 0.0288\n",
      "Saving checkpoints at 67 epochs.\n",
      "Epoch:[68/100], Step: [0] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[68/100], Step: [50] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[68/100], Step: [100] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[68/100], Step: [150] learning_rate: 0.0001, loss: 0.0269\n",
      "Epoch:[68/100], Step: [200] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[68/100], Step: [250] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[68/100], Step: [300] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[68/100], Step: [350] learning_rate: 0.0001, loss: 0.0256\n",
      "Epoch:[68/100], Step: [400] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[68/100], Step: [450] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[68/100], Step: [500] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[68/100], Step: [550] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[68/100], Step: [600] learning_rate: 0.0001, loss: 0.0284\n",
      "Average MSE loss on test dataset: 0.0288\n",
      "Saving checkpoints at 68 epochs.\n",
      "Epoch:[69/100], Step: [0] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[69/100], Step: [50] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[69/100], Step: [100] learning_rate: 0.0001, loss: 0.0256\n",
      "Epoch:[69/100], Step: [150] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[69/100], Step: [200] learning_rate: 0.0001, loss: 0.0264\n",
      "Epoch:[69/100], Step: [250] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[69/100], Step: [300] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[69/100], Step: [350] learning_rate: 0.0001, loss: 0.0255\n",
      "Epoch:[69/100], Step: [400] learning_rate: 0.0001, loss: 0.0280\n",
      "Epoch:[69/100], Step: [450] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[69/100], Step: [500] learning_rate: 0.0001, loss: 0.0258\n",
      "Epoch:[69/100], Step: [550] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[69/100], Step: [600] learning_rate: 0.0001, loss: 0.0280\n",
      "Average MSE loss on test dataset: 0.0287\n",
      "Saving checkpoints at 69 epochs.\n",
      "Epoch:[70/100], Step: [0] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[70/100], Step: [50] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[70/100], Step: [100] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[70/100], Step: [150] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[70/100], Step: [200] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[70/100], Step: [250] learning_rate: 0.0001, loss: 0.0277\n",
      "Epoch:[70/100], Step: [300] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[70/100], Step: [350] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[70/100], Step: [400] learning_rate: 0.0001, loss: 0.0329\n",
      "Epoch:[70/100], Step: [450] learning_rate: 0.0001, loss: 0.0263\n",
      "Epoch:[70/100], Step: [500] learning_rate: 0.0001, loss: 0.0284\n",
      "Epoch:[70/100], Step: [550] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[70/100], Step: [600] learning_rate: 0.0001, loss: 0.0303\n",
      "Average MSE loss on test dataset: 0.0287\n",
      "Saving checkpoints at 70 epochs.\n",
      "Epoch:[71/100], Step: [0] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[71/100], Step: [50] learning_rate: 0.0001, loss: 0.0269\n",
      "Epoch:[71/100], Step: [100] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[71/100], Step: [150] learning_rate: 0.0001, loss: 0.0264\n",
      "Epoch:[71/100], Step: [200] learning_rate: 0.0001, loss: 0.0271\n",
      "Epoch:[71/100], Step: [250] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[71/100], Step: [300] learning_rate: 0.0001, loss: 0.0259\n",
      "Epoch:[71/100], Step: [350] learning_rate: 0.0001, loss: 0.0274\n",
      "Epoch:[71/100], Step: [400] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[71/100], Step: [450] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[71/100], Step: [500] learning_rate: 0.0001, loss: 0.0263\n",
      "Epoch:[71/100], Step: [550] learning_rate: 0.0001, loss: 0.0276\n",
      "Epoch:[71/100], Step: [600] learning_rate: 0.0001, loss: 0.0282\n",
      "Average MSE loss on test dataset: 0.0285\n",
      "Saving checkpoints at 71 epochs.\n",
      "Epoch:[72/100], Step: [0] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[72/100], Step: [50] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[72/100], Step: [100] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[72/100], Step: [150] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[72/100], Step: [200] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[72/100], Step: [250] learning_rate: 0.0001, loss: 0.0258\n",
      "Epoch:[72/100], Step: [300] learning_rate: 0.0001, loss: 0.0258\n",
      "Epoch:[72/100], Step: [350] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[72/100], Step: [400] learning_rate: 0.0001, loss: 0.0284\n",
      "Epoch:[72/100], Step: [450] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[72/100], Step: [500] learning_rate: 0.0001, loss: 0.0253\n",
      "Epoch:[72/100], Step: [550] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[72/100], Step: [600] learning_rate: 0.0001, loss: 0.0287\n",
      "Average MSE loss on test dataset: 0.0285\n",
      "Saving checkpoints at 72 epochs.\n",
      "Epoch:[73/100], Step: [0] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[73/100], Step: [50] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[73/100], Step: [100] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[73/100], Step: [150] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[73/100], Step: [200] learning_rate: 0.0001, loss: 0.0263\n",
      "Epoch:[73/100], Step: [250] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[73/100], Step: [300] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[73/100], Step: [350] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[73/100], Step: [400] learning_rate: 0.0001, loss: 0.0284\n",
      "Epoch:[73/100], Step: [450] learning_rate: 0.0001, loss: 0.0266\n",
      "Epoch:[73/100], Step: [500] learning_rate: 0.0001, loss: 0.0242\n",
      "Epoch:[73/100], Step: [550] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[73/100], Step: [600] learning_rate: 0.0001, loss: 0.0272\n",
      "Average MSE loss on test dataset: 0.0284\n",
      "Saving checkpoints at 73 epochs.\n",
      "Epoch:[74/100], Step: [0] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[74/100], Step: [50] learning_rate: 0.0001, loss: 0.0268\n",
      "Epoch:[74/100], Step: [100] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[74/100], Step: [150] learning_rate: 0.0001, loss: 0.0277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[74/100], Step: [200] learning_rate: 0.0001, loss: 0.0261\n",
      "Epoch:[74/100], Step: [250] learning_rate: 0.0001, loss: 0.0273\n",
      "Epoch:[74/100], Step: [300] learning_rate: 0.0001, loss: 0.0280\n",
      "Epoch:[74/100], Step: [350] learning_rate: 0.0001, loss: 0.0277\n",
      "Epoch:[74/100], Step: [400] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[74/100], Step: [450] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[74/100], Step: [500] learning_rate: 0.0001, loss: 0.0272\n",
      "Epoch:[74/100], Step: [550] learning_rate: 0.0001, loss: 0.0269\n",
      "Epoch:[74/100], Step: [600] learning_rate: 0.0001, loss: 0.0279\n",
      "Average MSE loss on test dataset: 0.0284\n",
      "Saving checkpoints at 74 epochs.\n",
      "Epoch:[75/100], Step: [0] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[75/100], Step: [50] learning_rate: 0.0001, loss: 0.0271\n",
      "Epoch:[75/100], Step: [100] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[75/100], Step: [150] learning_rate: 0.0001, loss: 0.0269\n",
      "Epoch:[75/100], Step: [200] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[75/100], Step: [250] learning_rate: 0.0001, loss: 0.0327\n",
      "Epoch:[75/100], Step: [300] learning_rate: 0.0001, loss: 0.0308\n",
      "Epoch:[75/100], Step: [350] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[75/100], Step: [400] learning_rate: 0.0001, loss: 0.0256\n",
      "Epoch:[75/100], Step: [450] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[75/100], Step: [500] learning_rate: 0.0001, loss: 0.0267\n",
      "Epoch:[75/100], Step: [550] learning_rate: 0.0001, loss: 0.0260\n",
      "Epoch:[75/100], Step: [600] learning_rate: 0.0001, loss: 0.0297\n",
      "Average MSE loss on test dataset: 0.0283\n",
      "Saving checkpoints at 75 epochs.\n",
      "Epoch:[76/100], Step: [0] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[76/100], Step: [50] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[76/100], Step: [100] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[76/100], Step: [150] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[76/100], Step: [200] learning_rate: 0.0001, loss: 0.0270\n",
      "Epoch:[76/100], Step: [250] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[76/100], Step: [300] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[76/100], Step: [350] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[76/100], Step: [400] learning_rate: 0.0001, loss: 0.0277\n",
      "Epoch:[76/100], Step: [450] learning_rate: 0.0001, loss: 0.0284\n",
      "Epoch:[76/100], Step: [500] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[76/100], Step: [550] learning_rate: 0.0001, loss: 0.0272\n",
      "Epoch:[76/100], Step: [600] learning_rate: 0.0001, loss: 0.0257\n",
      "Average MSE loss on test dataset: 0.0283\n",
      "Saving checkpoints at 76 epochs.\n",
      "Epoch:[77/100], Step: [0] learning_rate: 0.0001, loss: 0.0276\n",
      "Epoch:[77/100], Step: [50] learning_rate: 0.0001, loss: 0.0271\n",
      "Epoch:[77/100], Step: [100] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[77/100], Step: [150] learning_rate: 0.0001, loss: 0.0276\n",
      "Epoch:[77/100], Step: [200] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[77/100], Step: [250] learning_rate: 0.0001, loss: 0.0253\n",
      "Epoch:[77/100], Step: [300] learning_rate: 0.0001, loss: 0.0263\n",
      "Epoch:[77/100], Step: [350] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[77/100], Step: [400] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[77/100], Step: [450] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[77/100], Step: [500] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[77/100], Step: [550] learning_rate: 0.0001, loss: 0.0276\n",
      "Epoch:[77/100], Step: [600] learning_rate: 0.0001, loss: 0.0272\n",
      "Average MSE loss on test dataset: 0.0281\n",
      "Saving checkpoints at 77 epochs.\n",
      "Epoch:[78/100], Step: [0] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[78/100], Step: [50] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[78/100], Step: [100] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[78/100], Step: [150] learning_rate: 0.0001, loss: 0.0267\n",
      "Epoch:[78/100], Step: [200] learning_rate: 0.0001, loss: 0.0261\n",
      "Epoch:[78/100], Step: [250] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[78/100], Step: [300] learning_rate: 0.0001, loss: 0.0271\n",
      "Epoch:[78/100], Step: [350] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[78/100], Step: [400] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[78/100], Step: [450] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[78/100], Step: [500] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[78/100], Step: [550] learning_rate: 0.0001, loss: 0.0273\n",
      "Epoch:[78/100], Step: [600] learning_rate: 0.0001, loss: 0.0285\n",
      "Average MSE loss on test dataset: 0.0281\n",
      "Saving checkpoints at 78 epochs.\n",
      "Epoch:[79/100], Step: [0] learning_rate: 0.0001, loss: 0.0260\n",
      "Epoch:[79/100], Step: [50] learning_rate: 0.0001, loss: 0.0260\n",
      "Epoch:[79/100], Step: [100] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[79/100], Step: [150] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[79/100], Step: [200] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[79/100], Step: [250] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[79/100], Step: [300] learning_rate: 0.0001, loss: 0.0270\n",
      "Epoch:[79/100], Step: [350] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[79/100], Step: [400] learning_rate: 0.0001, loss: 0.0261\n",
      "Epoch:[79/100], Step: [450] learning_rate: 0.0001, loss: 0.0253\n",
      "Epoch:[79/100], Step: [500] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[79/100], Step: [550] learning_rate: 0.0001, loss: 0.0262\n",
      "Epoch:[79/100], Step: [600] learning_rate: 0.0001, loss: 0.0237\n",
      "Average MSE loss on test dataset: 0.0281\n",
      "Saving checkpoints at 79 epochs.\n",
      "Epoch:[80/100], Step: [0] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[80/100], Step: [50] learning_rate: 0.0001, loss: 0.0268\n",
      "Epoch:[80/100], Step: [100] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[80/100], Step: [150] learning_rate: 0.0001, loss: 0.0274\n",
      "Epoch:[80/100], Step: [200] learning_rate: 0.0001, loss: 0.0272\n",
      "Epoch:[80/100], Step: [250] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[80/100], Step: [300] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[80/100], Step: [350] learning_rate: 0.0001, loss: 0.0258\n",
      "Epoch:[80/100], Step: [400] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[80/100], Step: [450] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[80/100], Step: [500] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[80/100], Step: [550] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[80/100], Step: [600] learning_rate: 0.0001, loss: 0.0284\n",
      "Average MSE loss on test dataset: 0.0281\n",
      "Saving checkpoints at 80 epochs.\n",
      "Epoch:[81/100], Step: [0] learning_rate: 0.0001, loss: 0.0249\n",
      "Epoch:[81/100], Step: [50] learning_rate: 0.0001, loss: 0.0259\n",
      "Epoch:[81/100], Step: [100] learning_rate: 0.0001, loss: 0.0268\n",
      "Epoch:[81/100], Step: [150] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[81/100], Step: [200] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[81/100], Step: [250] learning_rate: 0.0001, loss: 0.0276\n",
      "Epoch:[81/100], Step: [300] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[81/100], Step: [350] learning_rate: 0.0001, loss: 0.0246\n",
      "Epoch:[81/100], Step: [400] learning_rate: 0.0001, loss: 0.0265\n",
      "Epoch:[81/100], Step: [450] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[81/100], Step: [500] learning_rate: 0.0001, loss: 0.0264\n",
      "Epoch:[81/100], Step: [550] learning_rate: 0.0001, loss: 0.0276\n",
      "Epoch:[81/100], Step: [600] learning_rate: 0.0001, loss: 0.0290\n",
      "Average MSE loss on test dataset: 0.0279\n",
      "Saving checkpoints at 81 epochs.\n",
      "Epoch:[82/100], Step: [0] learning_rate: 0.0001, loss: 0.0261\n",
      "Epoch:[82/100], Step: [50] learning_rate: 0.0001, loss: 0.0266\n",
      "Epoch:[82/100], Step: [100] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[82/100], Step: [150] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[82/100], Step: [200] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[82/100], Step: [250] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[82/100], Step: [300] learning_rate: 0.0001, loss: 0.0264\n",
      "Epoch:[82/100], Step: [350] learning_rate: 0.0001, loss: 0.0263\n",
      "Epoch:[82/100], Step: [400] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[82/100], Step: [450] learning_rate: 0.0001, loss: 0.0276\n",
      "Epoch:[82/100], Step: [500] learning_rate: 0.0001, loss: 0.0266\n",
      "Epoch:[82/100], Step: [550] learning_rate: 0.0001, loss: 0.0277\n",
      "Epoch:[82/100], Step: [600] learning_rate: 0.0001, loss: 0.0255\n",
      "Average MSE loss on test dataset: 0.0279\n",
      "Saving checkpoints at 82 epochs.\n",
      "Epoch:[83/100], Step: [0] learning_rate: 0.0001, loss: 0.0264\n",
      "Epoch:[83/100], Step: [50] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[83/100], Step: [100] learning_rate: 0.0001, loss: 0.0265\n",
      "Epoch:[83/100], Step: [150] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[83/100], Step: [200] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[83/100], Step: [250] learning_rate: 0.0001, loss: 0.0294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[83/100], Step: [300] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[83/100], Step: [350] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[83/100], Step: [400] learning_rate: 0.0001, loss: 0.0268\n",
      "Epoch:[83/100], Step: [450] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[83/100], Step: [500] learning_rate: 0.0001, loss: 0.0245\n",
      "Epoch:[83/100], Step: [550] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[83/100], Step: [600] learning_rate: 0.0001, loss: 0.0297\n",
      "Average MSE loss on test dataset: 0.0278\n",
      "Saving checkpoints at 83 epochs.\n",
      "Epoch:[84/100], Step: [0] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[84/100], Step: [50] learning_rate: 0.0001, loss: 0.0257\n",
      "Epoch:[84/100], Step: [100] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[84/100], Step: [150] learning_rate: 0.0001, loss: 0.0274\n",
      "Epoch:[84/100], Step: [200] learning_rate: 0.0001, loss: 0.0249\n",
      "Epoch:[84/100], Step: [250] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[84/100], Step: [300] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[84/100], Step: [350] learning_rate: 0.0001, loss: 0.0264\n",
      "Epoch:[84/100], Step: [400] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[84/100], Step: [450] learning_rate: 0.0001, loss: 0.0257\n",
      "Epoch:[84/100], Step: [500] learning_rate: 0.0001, loss: 0.0263\n",
      "Epoch:[84/100], Step: [550] learning_rate: 0.0001, loss: 0.0262\n",
      "Epoch:[84/100], Step: [600] learning_rate: 0.0001, loss: 0.0313\n",
      "Average MSE loss on test dataset: 0.0278\n",
      "Saving checkpoints at 84 epochs.\n",
      "Epoch:[85/100], Step: [0] learning_rate: 0.0001, loss: 0.0271\n",
      "Epoch:[85/100], Step: [50] learning_rate: 0.0001, loss: 0.0238\n",
      "Epoch:[85/100], Step: [100] learning_rate: 0.0001, loss: 0.0267\n",
      "Epoch:[85/100], Step: [150] learning_rate: 0.0001, loss: 0.0268\n",
      "Epoch:[85/100], Step: [200] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[85/100], Step: [250] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[85/100], Step: [300] learning_rate: 0.0001, loss: 0.0263\n",
      "Epoch:[85/100], Step: [350] learning_rate: 0.0001, loss: 0.0274\n",
      "Epoch:[85/100], Step: [400] learning_rate: 0.0001, loss: 0.0274\n",
      "Epoch:[85/100], Step: [450] learning_rate: 0.0001, loss: 0.0271\n",
      "Epoch:[85/100], Step: [500] learning_rate: 0.0001, loss: 0.0257\n",
      "Epoch:[85/100], Step: [550] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[85/100], Step: [600] learning_rate: 0.0001, loss: 0.0299\n",
      "Average MSE loss on test dataset: 0.0280\n",
      "Saving checkpoints at 85 epochs.\n",
      "Epoch:[86/100], Step: [0] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[86/100], Step: [50] learning_rate: 0.0001, loss: 0.0284\n",
      "Epoch:[86/100], Step: [100] learning_rate: 0.0001, loss: 0.0246\n",
      "Epoch:[86/100], Step: [150] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[86/100], Step: [200] learning_rate: 0.0001, loss: 0.0251\n",
      "Epoch:[86/100], Step: [250] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[86/100], Step: [300] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[86/100], Step: [350] learning_rate: 0.0001, loss: 0.0268\n",
      "Epoch:[86/100], Step: [400] learning_rate: 0.0001, loss: 0.0272\n",
      "Epoch:[86/100], Step: [450] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[86/100], Step: [500] learning_rate: 0.0001, loss: 0.0273\n",
      "Epoch:[86/100], Step: [550] learning_rate: 0.0001, loss: 0.0260\n",
      "Epoch:[86/100], Step: [600] learning_rate: 0.0001, loss: 0.0276\n",
      "Average MSE loss on test dataset: 0.0279\n",
      "Saving checkpoints at 86 epochs.\n",
      "Epoch:[87/100], Step: [0] learning_rate: 0.0001, loss: 0.0256\n",
      "Epoch:[87/100], Step: [50] learning_rate: 0.0001, loss: 0.0259\n",
      "Epoch:[87/100], Step: [100] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[87/100], Step: [150] learning_rate: 0.0001, loss: 0.0280\n",
      "Epoch:[87/100], Step: [200] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[87/100], Step: [250] learning_rate: 0.0001, loss: 0.0270\n",
      "Epoch:[87/100], Step: [300] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[87/100], Step: [350] learning_rate: 0.0001, loss: 0.0269\n",
      "Epoch:[87/100], Step: [400] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[87/100], Step: [450] learning_rate: 0.0001, loss: 0.0272\n",
      "Epoch:[87/100], Step: [500] learning_rate: 0.0001, loss: 0.0272\n",
      "Epoch:[87/100], Step: [550] learning_rate: 0.0001, loss: 0.0273\n",
      "Epoch:[87/100], Step: [600] learning_rate: 0.0001, loss: 0.0288\n",
      "Average MSE loss on test dataset: 0.0276\n",
      "Saving checkpoints at 87 epochs.\n",
      "Epoch:[88/100], Step: [0] learning_rate: 0.0001, loss: 0.0240\n",
      "Epoch:[88/100], Step: [50] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[88/100], Step: [100] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[88/100], Step: [150] learning_rate: 0.0001, loss: 0.0240\n",
      "Epoch:[88/100], Step: [200] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[88/100], Step: [250] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[88/100], Step: [300] learning_rate: 0.0001, loss: 0.0267\n",
      "Epoch:[88/100], Step: [350] learning_rate: 0.0001, loss: 0.0256\n",
      "Epoch:[88/100], Step: [400] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[88/100], Step: [450] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[88/100], Step: [500] learning_rate: 0.0001, loss: 0.0265\n",
      "Epoch:[88/100], Step: [550] learning_rate: 0.0001, loss: 0.0257\n",
      "Epoch:[88/100], Step: [600] learning_rate: 0.0001, loss: 0.0276\n",
      "Average MSE loss on test dataset: 0.0277\n",
      "Saving checkpoints at 88 epochs.\n",
      "Epoch:[89/100], Step: [0] learning_rate: 0.0001, loss: 0.0259\n",
      "Epoch:[89/100], Step: [50] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[89/100], Step: [100] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[89/100], Step: [150] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[89/100], Step: [200] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[89/100], Step: [250] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[89/100], Step: [300] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[89/100], Step: [350] learning_rate: 0.0001, loss: 0.0268\n",
      "Epoch:[89/100], Step: [400] learning_rate: 0.0001, loss: 0.0255\n",
      "Epoch:[89/100], Step: [450] learning_rate: 0.0001, loss: 0.0265\n",
      "Epoch:[89/100], Step: [500] learning_rate: 0.0001, loss: 0.0253\n",
      "Epoch:[89/100], Step: [550] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[89/100], Step: [600] learning_rate: 0.0001, loss: 0.0276\n",
      "Average MSE loss on test dataset: 0.0276\n",
      "Saving checkpoints at 89 epochs.\n",
      "Epoch:[90/100], Step: [0] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[90/100], Step: [50] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[90/100], Step: [100] learning_rate: 0.0001, loss: 0.0252\n",
      "Epoch:[90/100], Step: [150] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[90/100], Step: [200] learning_rate: 0.0001, loss: 0.0269\n",
      "Epoch:[90/100], Step: [250] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[90/100], Step: [300] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[90/100], Step: [350] learning_rate: 0.0001, loss: 0.0243\n",
      "Epoch:[90/100], Step: [400] learning_rate: 0.0001, loss: 0.0259\n",
      "Epoch:[90/100], Step: [450] learning_rate: 0.0001, loss: 0.0264\n",
      "Epoch:[90/100], Step: [500] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[90/100], Step: [550] learning_rate: 0.0001, loss: 0.0280\n",
      "Epoch:[90/100], Step: [600] learning_rate: 0.0001, loss: 0.0300\n",
      "Average MSE loss on test dataset: 0.0274\n",
      "Saving checkpoints at 90 epochs.\n",
      "Epoch:[91/100], Step: [0] learning_rate: 0.0001, loss: 0.0253\n",
      "Epoch:[91/100], Step: [50] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[91/100], Step: [100] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[91/100], Step: [150] learning_rate: 0.0001, loss: 0.0266\n",
      "Epoch:[91/100], Step: [200] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[91/100], Step: [250] learning_rate: 0.0001, loss: 0.0244\n",
      "Epoch:[91/100], Step: [300] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[91/100], Step: [350] learning_rate: 0.0001, loss: 0.0259\n",
      "Epoch:[91/100], Step: [400] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[91/100], Step: [450] learning_rate: 0.0001, loss: 0.0272\n",
      "Epoch:[91/100], Step: [500] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[91/100], Step: [550] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[91/100], Step: [600] learning_rate: 0.0001, loss: 0.0248\n",
      "Average MSE loss on test dataset: 0.0275\n",
      "Saving checkpoints at 91 epochs.\n",
      "Epoch:[92/100], Step: [0] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[92/100], Step: [50] learning_rate: 0.0001, loss: 0.0269\n",
      "Epoch:[92/100], Step: [100] learning_rate: 0.0001, loss: 0.0277\n",
      "Epoch:[92/100], Step: [150] learning_rate: 0.0001, loss: 0.0258\n",
      "Epoch:[92/100], Step: [200] learning_rate: 0.0001, loss: 0.0277\n",
      "Epoch:[92/100], Step: [250] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[92/100], Step: [300] learning_rate: 0.0001, loss: 0.0269\n",
      "Epoch:[92/100], Step: [350] learning_rate: 0.0001, loss: 0.0258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[92/100], Step: [400] learning_rate: 0.0001, loss: 0.0267\n",
      "Epoch:[92/100], Step: [450] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[92/100], Step: [500] learning_rate: 0.0001, loss: 0.0268\n",
      "Epoch:[92/100], Step: [550] learning_rate: 0.0001, loss: 0.0262\n",
      "Epoch:[92/100], Step: [600] learning_rate: 0.0001, loss: 0.0274\n",
      "Average MSE loss on test dataset: 0.0274\n",
      "Saving checkpoints at 92 epochs.\n",
      "Epoch:[93/100], Step: [0] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[93/100], Step: [50] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[93/100], Step: [100] learning_rate: 0.0001, loss: 0.0269\n",
      "Epoch:[93/100], Step: [150] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[93/100], Step: [200] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[93/100], Step: [250] learning_rate: 0.0001, loss: 0.0252\n",
      "Epoch:[93/100], Step: [300] learning_rate: 0.0001, loss: 0.0260\n",
      "Epoch:[93/100], Step: [350] learning_rate: 0.0001, loss: 0.0274\n",
      "Epoch:[93/100], Step: [400] learning_rate: 0.0001, loss: 0.0280\n",
      "Epoch:[93/100], Step: [450] learning_rate: 0.0001, loss: 0.0246\n",
      "Epoch:[93/100], Step: [500] learning_rate: 0.0001, loss: 0.0266\n",
      "Epoch:[93/100], Step: [550] learning_rate: 0.0001, loss: 0.0268\n",
      "Epoch:[93/100], Step: [600] learning_rate: 0.0001, loss: 0.0268\n",
      "Average MSE loss on test dataset: 0.0274\n",
      "Saving checkpoints at 93 epochs.\n",
      "Epoch:[94/100], Step: [0] learning_rate: 0.0001, loss: 0.0237\n",
      "Epoch:[94/100], Step: [50] learning_rate: 0.0001, loss: 0.0260\n",
      "Epoch:[94/100], Step: [100] learning_rate: 0.0001, loss: 0.0284\n",
      "Epoch:[94/100], Step: [150] learning_rate: 0.0001, loss: 0.0239\n",
      "Epoch:[94/100], Step: [200] learning_rate: 0.0001, loss: 0.0244\n",
      "Epoch:[94/100], Step: [250] learning_rate: 0.0001, loss: 0.0273\n",
      "Epoch:[94/100], Step: [300] learning_rate: 0.0001, loss: 0.0269\n",
      "Epoch:[94/100], Step: [350] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[94/100], Step: [400] learning_rate: 0.0001, loss: 0.0280\n",
      "Epoch:[94/100], Step: [450] learning_rate: 0.0001, loss: 0.0273\n",
      "Epoch:[94/100], Step: [500] learning_rate: 0.0001, loss: 0.0268\n",
      "Epoch:[94/100], Step: [550] learning_rate: 0.0001, loss: 0.0256\n",
      "Epoch:[94/100], Step: [600] learning_rate: 0.0001, loss: 0.0308\n",
      "Average MSE loss on test dataset: 0.0273\n",
      "Saving checkpoints at 94 epochs.\n",
      "Epoch:[95/100], Step: [0] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[95/100], Step: [50] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[95/100], Step: [100] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[95/100], Step: [150] learning_rate: 0.0001, loss: 0.0271\n",
      "Epoch:[95/100], Step: [200] learning_rate: 0.0001, loss: 0.0273\n",
      "Epoch:[95/100], Step: [250] learning_rate: 0.0001, loss: 0.0260\n",
      "Epoch:[95/100], Step: [300] learning_rate: 0.0001, loss: 0.0240\n",
      "Epoch:[95/100], Step: [350] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[95/100], Step: [400] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[95/100], Step: [450] learning_rate: 0.0001, loss: 0.0264\n",
      "Epoch:[95/100], Step: [500] learning_rate: 0.0001, loss: 0.0253\n",
      "Epoch:[95/100], Step: [550] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[95/100], Step: [600] learning_rate: 0.0001, loss: 0.0270\n",
      "Average MSE loss on test dataset: 0.0273\n",
      "Saving checkpoints at 95 epochs.\n",
      "Epoch:[96/100], Step: [0] learning_rate: 0.0001, loss: 0.0263\n",
      "Epoch:[96/100], Step: [50] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[96/100], Step: [100] learning_rate: 0.0001, loss: 0.0264\n",
      "Epoch:[96/100], Step: [150] learning_rate: 0.0001, loss: 0.0257\n",
      "Epoch:[96/100], Step: [200] learning_rate: 0.0001, loss: 0.0273\n",
      "Epoch:[96/100], Step: [250] learning_rate: 0.0001, loss: 0.0256\n",
      "Epoch:[96/100], Step: [300] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[96/100], Step: [350] learning_rate: 0.0001, loss: 0.0254\n",
      "Epoch:[96/100], Step: [400] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[96/100], Step: [450] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[96/100], Step: [500] learning_rate: 0.0001, loss: 0.0276\n",
      "Epoch:[96/100], Step: [550] learning_rate: 0.0001, loss: 0.0267\n",
      "Epoch:[96/100], Step: [600] learning_rate: 0.0001, loss: 0.0287\n",
      "Average MSE loss on test dataset: 0.0273\n",
      "Saving checkpoints at 96 epochs.\n",
      "Epoch:[97/100], Step: [0] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[97/100], Step: [50] learning_rate: 0.0001, loss: 0.0273\n",
      "Epoch:[97/100], Step: [100] learning_rate: 0.0001, loss: 0.0262\n",
      "Epoch:[97/100], Step: [150] learning_rate: 0.0001, loss: 0.0266\n",
      "Epoch:[97/100], Step: [200] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[97/100], Step: [250] learning_rate: 0.0001, loss: 0.0256\n",
      "Epoch:[97/100], Step: [300] learning_rate: 0.0001, loss: 0.0273\n",
      "Epoch:[97/100], Step: [350] learning_rate: 0.0001, loss: 0.0284\n",
      "Epoch:[97/100], Step: [400] learning_rate: 0.0001, loss: 0.0270\n",
      "Epoch:[97/100], Step: [450] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[97/100], Step: [500] learning_rate: 0.0001, loss: 0.0257\n",
      "Epoch:[97/100], Step: [550] learning_rate: 0.0001, loss: 0.0245\n",
      "Epoch:[97/100], Step: [600] learning_rate: 0.0001, loss: 0.0261\n",
      "Average MSE loss on test dataset: 0.0273\n",
      "Saving checkpoints at 97 epochs.\n",
      "Epoch:[98/100], Step: [0] learning_rate: 0.0001, loss: 0.0248\n",
      "Epoch:[98/100], Step: [50] learning_rate: 0.0001, loss: 0.0262\n",
      "Epoch:[98/100], Step: [100] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[98/100], Step: [150] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[98/100], Step: [200] learning_rate: 0.0001, loss: 0.0257\n",
      "Epoch:[98/100], Step: [250] learning_rate: 0.0001, loss: 0.0277\n",
      "Epoch:[98/100], Step: [300] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[98/100], Step: [350] learning_rate: 0.0001, loss: 0.0244\n",
      "Epoch:[98/100], Step: [400] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[98/100], Step: [450] learning_rate: 0.0001, loss: 0.0260\n",
      "Epoch:[98/100], Step: [500] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[98/100], Step: [550] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[98/100], Step: [600] learning_rate: 0.0001, loss: 0.0281\n",
      "Average MSE loss on test dataset: 0.0271\n",
      "Saving checkpoints at 98 epochs.\n",
      "Epoch:[99/100], Step: [0] learning_rate: 0.0001, loss: 0.0240\n",
      "Epoch:[99/100], Step: [50] learning_rate: 0.0001, loss: 0.0234\n",
      "Epoch:[99/100], Step: [100] learning_rate: 0.0001, loss: 0.0273\n",
      "Epoch:[99/100], Step: [150] learning_rate: 0.0001, loss: 0.0272\n",
      "Epoch:[99/100], Step: [200] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[99/100], Step: [250] learning_rate: 0.0001, loss: 0.0258\n",
      "Epoch:[99/100], Step: [300] learning_rate: 0.0001, loss: 0.0267\n",
      "Epoch:[99/100], Step: [350] learning_rate: 0.0001, loss: 0.0256\n",
      "Epoch:[99/100], Step: [400] learning_rate: 0.0001, loss: 0.0267\n",
      "Epoch:[99/100], Step: [450] learning_rate: 0.0001, loss: 0.0280\n",
      "Epoch:[99/100], Step: [500] learning_rate: 0.0001, loss: 0.0280\n",
      "Epoch:[99/100], Step: [550] learning_rate: 0.0001, loss: 0.0239\n",
      "Epoch:[99/100], Step: [600] learning_rate: 0.0001, loss: 0.0270\n",
      "Average MSE loss on test dataset: 0.0271\n",
      "Saving checkpoints at 99 epochs.\n",
      "Epoch:[100/100], Step: [0] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[100/100], Step: [50] learning_rate: 0.0001, loss: 0.0284\n",
      "Epoch:[100/100], Step: [100] learning_rate: 0.0001, loss: 0.0252\n",
      "Epoch:[100/100], Step: [150] learning_rate: 0.0001, loss: 0.0273\n",
      "Epoch:[100/100], Step: [200] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[100/100], Step: [250] learning_rate: 0.0001, loss: 0.0264\n",
      "Epoch:[100/100], Step: [300] learning_rate: 0.0001, loss: 0.0245\n",
      "Epoch:[100/100], Step: [350] learning_rate: 0.0001, loss: 0.0267\n",
      "Epoch:[100/100], Step: [400] learning_rate: 0.0001, loss: 0.0257\n",
      "Epoch:[100/100], Step: [450] learning_rate: 0.0001, loss: 0.0277\n",
      "Epoch:[100/100], Step: [500] learning_rate: 0.0001, loss: 0.0258\n",
      "Epoch:[100/100], Step: [550] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[100/100], Step: [600] learning_rate: 0.0001, loss: 0.0278\n",
      "Average MSE loss on test dataset: 0.0269\n",
      "Saving checkpoints at 100 epochs.\n",
      "Training Done!\n"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "    net = EncoderDecoderConvLSTM(num_hidden_dim=args.n_hidden_dim, in_channel=1)\n",
    "    criterion = create_criterion()\n",
    "    \n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                     dataset=train_set,\n",
    "                     batch_size=args.batch_size,\n",
    "                     shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "                    dataset=test_set,\n",
    "                    batch_size=args.batch_size,\n",
    "                    shuffle=False)\n",
    "    \n",
    "    optimizer = create_optimizer(net, args.lr)\n",
    "    \n",
    "    # send net to gpu\n",
    "    net.to(args.device)\n",
    "    \n",
    "    if args.load:\n",
    "        net.load_state_dict(torch.load(os.path.join(args.ckpt_dir, 'net_best.pth')))\n",
    "        print(\"========== load checkpoints successfully! ==========\")\n",
    "    \n",
    "    if args.eval:\n",
    "        print(\"========== evaluating... ==========\")\n",
    "        evaluate(net, test_loader, criterion, args)\n",
    "        print(\"========== evaluation done! ==========\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        # traning function\n",
    "        train(net, train_loader, criterion, optimizer, epoch, args)\n",
    "        \n",
    "        # Evaluation and visualization\n",
    "        if epoch % args.eval_epoch == 0:\n",
    "            cur_loss = evaluate_epoch(net, test_loader, criterion, epoch, args)\n",
    "            # checkpointing\n",
    "            checkpoint(net, epoch, cur_loss ,args)\n",
    "\n",
    "    print('Training Done!')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--lr', default=1e-4, type=float, help='learning rate')\n",
    "    parser.add_argument('--batch_size', default=16, type=int, help='batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=100, help='number of epochs to train for')\n",
    "    parser.add_argument('--eval_epoch', type=int, default=1, help='number of epochs to evaluate once')\n",
    "    parser.add_argument('--n_hidden_dim', type=int, default=64, help='number of hidden dim for ConvLSTM cells')\n",
    "    parser.add_argument('--n_steps_ahead', type=int, default=10, help='length of predicted sequences')\n",
    "    parser.add_argument('--ckpt_dir', type=str, default='./ckpt', help='where you save trained model')\n",
    "    parser.add_argument('--load', type=bool, default=False, help='whether to load the previsous checkpoint')\n",
    "    parser.add_argument('--eval', type=bool, default=False, help='whether to evaluate the whole test set without training')\n",
    "    \n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if not os.path.exists(args.ckpt_dir):\n",
    "        os.makedirs(args.ckpt_dir)\n",
    "        \n",
    "    args.best_loss = float('inf')\n",
    "\n",
    "    # fix the random seed\n",
    "    random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    main(args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d306b71b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc0488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04095a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
